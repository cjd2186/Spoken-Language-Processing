{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df2e927",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* 7 speakers\n",
    "    * 'cc','cl', 'gg', 'jg', 'mf', 'mk', 'mm'\n",
    "* 15 emotions\n",
    "    * 'anxiety', 'boredom', 'cold-anger', 'contempt', 'despair', 'disgust', 'elation', 'happy', 'hot-anger', 'interest', 'neutral', 'panic', 'pride', 'sadness', 'shame'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86261f11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/audeering/opensmile.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4efb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!brew install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486d62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd opensmile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca0f44a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!bash build.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e09a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61442bf7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!ls -l config/is09-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28639752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!find ../hw3_speech_files/ -type f -name \"*.wav\" -exec sh -c 'for file; do base=$(basename \"$file\" \".wav\");  ./build/progsrc/smilextract/SMILExtract -l 1 -C ./config/is09-13/IS09_emotion.conf -I \"$file\" -instname \"$base\" -csvoutput \"../baseline_features/${base}.csv\";  done' sh {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e63bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! find ../hw3_speech_files/ -type f -name \"*.wav\" -exec sh -c 'for file; do base=$(basename \"$file\" \".wav\"); ./build/progsrc/smilextract/SMILExtract -l 1 -C ./config/is09-13/IS09_emotion.conf -I $file -classlabel \"$classlabel\" -csvoutput \"../baseline_features/${base}.csv\"; done' sh {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee58555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2324 CSV files into combined_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "input_dir = \"../baseline_features/\"  \n",
    "output_file = \"combined_features.csv\"\n",
    "\n",
    "csv_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "\n",
    "combined_df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Combined {len(csv_files)} CSV files into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4e6d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2324 entries, 0 to 2323\n",
      "Columns: 386 entries, name to F0_sma_de_kurtosis\n",
      "dtypes: float64(320), int64(65), object(1)\n",
      "memory usage: 6.8+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>frameTime</th>\n",
       "      <th>pcm_RMSenergy_sma_max</th>\n",
       "      <th>pcm_RMSenergy_sma_min</th>\n",
       "      <th>pcm_RMSenergy_sma_range</th>\n",
       "      <th>pcm_RMSenergy_sma_maxPos</th>\n",
       "      <th>pcm_RMSenergy_sma_minPos</th>\n",
       "      <th>pcm_RMSenergy_sma_amean</th>\n",
       "      <th>pcm_RMSenergy_sma_linregc1</th>\n",
       "      <th>pcm_RMSenergy_sma_linregc2</th>\n",
       "      <th>...</th>\n",
       "      <th>F0_sma_de_range</th>\n",
       "      <th>F0_sma_de_maxPos</th>\n",
       "      <th>F0_sma_de_minPos</th>\n",
       "      <th>F0_sma_de_amean</th>\n",
       "      <th>F0_sma_de_linregc1</th>\n",
       "      <th>F0_sma_de_linregc2</th>\n",
       "      <th>F0_sma_de_linregerrQ</th>\n",
       "      <th>F0_sma_de_stddev</th>\n",
       "      <th>F0_sma_de_skewness</th>\n",
       "      <th>F0_sma_de_kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'mm_001_happy_2353.51_three-hundred-nine'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.010696</td>\n",
       "      <td>111</td>\n",
       "      <td>165</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>-8.324649e-06</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>...</td>\n",
       "      <td>181.94580</td>\n",
       "      <td>85</td>\n",
       "      <td>92</td>\n",
       "      <td>8.510025e-08</td>\n",
       "      <td>-0.033835</td>\n",
       "      <td>3.045167</td>\n",
       "      <td>1062.67900</td>\n",
       "      <td>32.646660</td>\n",
       "      <td>-0.090631</td>\n",
       "      <td>4.245496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'mm_001_panic_3395.30_one-thousand-three'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076285</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.076251</td>\n",
       "      <td>32</td>\n",
       "      <td>134</td>\n",
       "      <td>0.011488</td>\n",
       "      <td>-1.106844e-04</td>\n",
       "      <td>0.019789</td>\n",
       "      <td>...</td>\n",
       "      <td>245.74430</td>\n",
       "      <td>85</td>\n",
       "      <td>68</td>\n",
       "      <td>3.852591e-08</td>\n",
       "      <td>-0.097880</td>\n",
       "      <td>7.340973</td>\n",
       "      <td>1683.38700</td>\n",
       "      <td>41.250330</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>4.717154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'cc_001_panic_861.97_Two-thousand-five'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020006</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.019996</td>\n",
       "      <td>39</td>\n",
       "      <td>1853</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>-2.645859e-07</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>...</td>\n",
       "      <td>181.56010</td>\n",
       "      <td>1909</td>\n",
       "      <td>617</td>\n",
       "      <td>-9.201901e-09</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.034964</td>\n",
       "      <td>213.41610</td>\n",
       "      <td>14.608780</td>\n",
       "      <td>-0.116606</td>\n",
       "      <td>17.215480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'mf_001_contempt_3901.86_November-first'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015069</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.015026</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005252</td>\n",
       "      <td>2.256441e-05</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>...</td>\n",
       "      <td>87.32532</td>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "      <td>4.048617e-08</td>\n",
       "      <td>-0.029053</td>\n",
       "      <td>1.525273</td>\n",
       "      <td>188.63710</td>\n",
       "      <td>13.763260</td>\n",
       "      <td>-0.312293</td>\n",
       "      <td>6.684447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'cl_001_interest_1035.82_Ten-thousand-one'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016625</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.016514</td>\n",
       "      <td>55</td>\n",
       "      <td>137</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>-3.115808e-05</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>...</td>\n",
       "      <td>74.25951</td>\n",
       "      <td>95</td>\n",
       "      <td>99</td>\n",
       "      <td>1.372193e-08</td>\n",
       "      <td>-0.001866</td>\n",
       "      <td>0.128773</td>\n",
       "      <td>39.98518</td>\n",
       "      <td>6.323827</td>\n",
       "      <td>-0.006288</td>\n",
       "      <td>25.196500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name  frameTime  \\\n",
       "0   'mm_001_happy_2353.51_three-hundred-nine'        0.0   \n",
       "1   'mm_001_panic_3395.30_one-thousand-three'        0.0   \n",
       "2     'cc_001_panic_861.97_Two-thousand-five'        0.0   \n",
       "3    'mf_001_contempt_3901.86_November-first'        0.0   \n",
       "4  'cl_001_interest_1035.82_Ten-thousand-one'        0.0   \n",
       "\n",
       "   pcm_RMSenergy_sma_max  pcm_RMSenergy_sma_min  pcm_RMSenergy_sma_range  \\\n",
       "0               0.010760               0.000064                 0.010696   \n",
       "1               0.076285               0.000034                 0.076251   \n",
       "2               0.020006               0.000010                 0.019996   \n",
       "3               0.015069               0.000043                 0.015026   \n",
       "4               0.016625               0.000111                 0.016514   \n",
       "\n",
       "   pcm_RMSenergy_sma_maxPos  pcm_RMSenergy_sma_minPos  \\\n",
       "0                       111                       165   \n",
       "1                        32                       134   \n",
       "2                        39                      1853   \n",
       "3                        85                         0   \n",
       "4                        55                       137   \n",
       "\n",
       "   pcm_RMSenergy_sma_amean  pcm_RMSenergy_sma_linregc1  \\\n",
       "0                 0.002748               -8.324649e-06   \n",
       "1                 0.011488               -1.106844e-04   \n",
       "2                 0.000425               -2.645859e-07   \n",
       "3                 0.005252                2.256441e-05   \n",
       "4                 0.002422               -3.115808e-05   \n",
       "\n",
       "   pcm_RMSenergy_sma_linregc2  ...  F0_sma_de_range  F0_sma_de_maxPos  \\\n",
       "0                    0.003498  ...        181.94580                85   \n",
       "1                    0.019789  ...        245.74430                85   \n",
       "2                    0.000788  ...        181.56010              1909   \n",
       "3                    0.004067  ...         87.32532                62   \n",
       "4                    0.004572  ...         74.25951                95   \n",
       "\n",
       "   F0_sma_de_minPos  F0_sma_de_amean  F0_sma_de_linregc1  F0_sma_de_linregc2  \\\n",
       "0                92     8.510025e-08           -0.033835            3.045167   \n",
       "1                68     3.852591e-08           -0.097880            7.340973   \n",
       "2               617    -9.201901e-09           -0.000025            0.034964   \n",
       "3                71     4.048617e-08           -0.029053            1.525273   \n",
       "4                99     1.372193e-08           -0.001866            0.128773   \n",
       "\n",
       "   F0_sma_de_linregerrQ  F0_sma_de_stddev  F0_sma_de_skewness  \\\n",
       "0            1062.67900         32.646660           -0.090631   \n",
       "1            1683.38700         41.250330            0.000596   \n",
       "2             213.41610         14.608780           -0.116606   \n",
       "3             188.63710         13.763260           -0.312293   \n",
       "4              39.98518          6.323827           -0.006288   \n",
       "\n",
       "   F0_sma_de_kurtosis  \n",
       "0            4.245496  \n",
       "1            4.717154  \n",
       "2           17.215480  \n",
       "3            6.684447  \n",
       "4           25.196500  \n",
       "\n",
       "[5 rows x 386 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_smile=pd.read_csv(\"combined_features.csv\", sep=\";\")\n",
    "print(open_smile.info())\n",
    "\n",
    "#replace 0s with NaN to help with training\n",
    "open_smile.fillna(0.0, inplace=True)\n",
    "\n",
    "open_smile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b4f038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mm', '001', 'happy', '2353.51', 'three-hundred-nine']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_name = open_smile['name'][0].strip(\"[]'\\\"\")\n",
    "split_name = clean_name.split(\"_\")\n",
    "split_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "437552b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize using method 2\n",
    "def open_norm(df):\n",
    "    speakers= df['name'].apply(lambda l: l.split(\"_\")[0].strip(\"[]'\\\"\"))\n",
    "    emotions= df['name'].apply(lambda l: l.split(\"_\")[2].strip(\"[]'\\\"\"))\n",
    "    df.insert(1, 'emotion', emotions)\n",
    "    df.insert(2, 'speaker', speakers)\n",
    "    \n",
    "    #get bneutral utterances and normalize for each speaker\n",
    "    for speaker in df['speaker'].unique():\n",
    "        neutral_df=df[(df['speaker']==speaker) & (df['emotion']=='neutral')]\n",
    "        \n",
    "        feature_cols= df.columns[3:]\n",
    "        for col in feature_cols:\n",
    "            if str(col) not in ['speaker', 'emotion']:\n",
    "                speaker_mean= neutral_df[col].mean()\n",
    "                df[col]= df[col] - speaker_mean\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf89d42d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>emotion</th>\n",
       "      <th>speaker</th>\n",
       "      <th>frameTime</th>\n",
       "      <th>pcm_RMSenergy_sma_max</th>\n",
       "      <th>pcm_RMSenergy_sma_min</th>\n",
       "      <th>pcm_RMSenergy_sma_range</th>\n",
       "      <th>pcm_RMSenergy_sma_maxPos</th>\n",
       "      <th>pcm_RMSenergy_sma_minPos</th>\n",
       "      <th>pcm_RMSenergy_sma_amean</th>\n",
       "      <th>...</th>\n",
       "      <th>F0_sma_de_range</th>\n",
       "      <th>F0_sma_de_maxPos</th>\n",
       "      <th>F0_sma_de_minPos</th>\n",
       "      <th>F0_sma_de_amean</th>\n",
       "      <th>F0_sma_de_linregc1</th>\n",
       "      <th>F0_sma_de_linregc2</th>\n",
       "      <th>F0_sma_de_linregerrQ</th>\n",
       "      <th>F0_sma_de_stddev</th>\n",
       "      <th>F0_sma_de_skewness</th>\n",
       "      <th>F0_sma_de_kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'mm_001_happy_2353.51_three-hundred-nine'</td>\n",
       "      <td>happy</td>\n",
       "      <td>mm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.036211</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.036197</td>\n",
       "      <td>78.0</td>\n",
       "      <td>72.222222</td>\n",
       "      <td>-0.005396</td>\n",
       "      <td>...</td>\n",
       "      <td>13.346391</td>\n",
       "      <td>46.111111</td>\n",
       "      <td>39.777778</td>\n",
       "      <td>9.411793e-08</td>\n",
       "      <td>-0.014334</td>\n",
       "      <td>1.637743</td>\n",
       "      <td>604.137643</td>\n",
       "      <td>12.327299</td>\n",
       "      <td>-0.478294</td>\n",
       "      <td>-7.692416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'mm_001_panic_3395.30_one-thousand-three'</td>\n",
       "      <td>panic</td>\n",
       "      <td>mm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029315</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.029359</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>41.222222</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>...</td>\n",
       "      <td>77.144891</td>\n",
       "      <td>46.111111</td>\n",
       "      <td>15.777778</td>\n",
       "      <td>4.754359e-08</td>\n",
       "      <td>-0.078379</td>\n",
       "      <td>5.933549</td>\n",
       "      <td>1224.845643</td>\n",
       "      <td>20.930969</td>\n",
       "      <td>-0.387068</td>\n",
       "      <td>-7.220758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'cc_001_panic_861.97_Two-thousand-five'</td>\n",
       "      <td>panic</td>\n",
       "      <td>cc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.026964</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.026896</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1760.222222</td>\n",
       "      <td>-0.007719</td>\n",
       "      <td>...</td>\n",
       "      <td>12.960691</td>\n",
       "      <td>1870.111111</td>\n",
       "      <td>564.777778</td>\n",
       "      <td>-1.842239e-10</td>\n",
       "      <td>0.019476</td>\n",
       "      <td>-1.372460</td>\n",
       "      <td>-245.125257</td>\n",
       "      <td>-5.710581</td>\n",
       "      <td>-0.504269</td>\n",
       "      <td>5.277568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'mf_001_contempt_3901.86_November-first'</td>\n",
       "      <td>contempt</td>\n",
       "      <td>mf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031901</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.031866</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-92.777778</td>\n",
       "      <td>-0.002893</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.274089</td>\n",
       "      <td>23.111111</td>\n",
       "      <td>18.777778</td>\n",
       "      <td>4.950385e-08</td>\n",
       "      <td>-0.009552</td>\n",
       "      <td>0.117849</td>\n",
       "      <td>-269.904257</td>\n",
       "      <td>-6.556101</td>\n",
       "      <td>-0.699956</td>\n",
       "      <td>-5.253465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'cl_001_interest_1035.82_Ten-thousand-one'</td>\n",
       "      <td>interest</td>\n",
       "      <td>cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.030345</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.030379</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.222222</td>\n",
       "      <td>-0.005722</td>\n",
       "      <td>...</td>\n",
       "      <td>-94.339899</td>\n",
       "      <td>56.111111</td>\n",
       "      <td>46.777778</td>\n",
       "      <td>2.273961e-08</td>\n",
       "      <td>0.017635</td>\n",
       "      <td>-1.278651</td>\n",
       "      <td>-418.556177</td>\n",
       "      <td>-13.995534</td>\n",
       "      <td>-0.393951</td>\n",
       "      <td>13.258588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>'gg_001_anxiety_861.72_Three-thousand-eight'</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>gg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.022373</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.022355</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.777778</td>\n",
       "      <td>-0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>46.652691</td>\n",
       "      <td>-34.888889</td>\n",
       "      <td>-43.222222</td>\n",
       "      <td>6.623814e-08</td>\n",
       "      <td>-0.008586</td>\n",
       "      <td>-0.017108</td>\n",
       "      <td>199.947643</td>\n",
       "      <td>5.354489</td>\n",
       "      <td>-0.139077</td>\n",
       "      <td>1.651818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>'cl_001_cold-anger_499.97_March-twenty-third'</td>\n",
       "      <td>cold-anger</td>\n",
       "      <td>cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063760</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.063743</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-67.777778</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>...</td>\n",
       "      <td>-84.113069</td>\n",
       "      <td>39.111111</td>\n",
       "      <td>73.777778</td>\n",
       "      <td>-1.037178e-08</td>\n",
       "      <td>0.015984</td>\n",
       "      <td>-1.117238</td>\n",
       "      <td>-342.813757</td>\n",
       "      <td>-9.560361</td>\n",
       "      <td>-0.405068</td>\n",
       "      <td>-1.692562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>'mm_001_elation_2227.76_nineteen-hundred'</td>\n",
       "      <td>elation</td>\n",
       "      <td>mm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011188</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-89.777778</td>\n",
       "      <td>-0.000509</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.602509</td>\n",
       "      <td>-28.888889</td>\n",
       "      <td>-35.222222</td>\n",
       "      <td>1.113053e-08</td>\n",
       "      <td>-0.037956</td>\n",
       "      <td>2.413479</td>\n",
       "      <td>416.147443</td>\n",
       "      <td>9.339169</td>\n",
       "      <td>-0.271780</td>\n",
       "      <td>-7.857798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>'jg_001_panic_425.32_August-fourteenth'</td>\n",
       "      <td>panic</td>\n",
       "      <td>jg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.014303</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.014280</td>\n",
       "      <td>42.0</td>\n",
       "      <td>106.222222</td>\n",
       "      <td>-0.006221</td>\n",
       "      <td>...</td>\n",
       "      <td>-168.599409</td>\n",
       "      <td>-38.888889</td>\n",
       "      <td>-52.222222</td>\n",
       "      <td>9.017677e-09</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>-1.407424</td>\n",
       "      <td>-458.541357</td>\n",
       "      <td>-20.319361</td>\n",
       "      <td>-0.387663</td>\n",
       "      <td>-11.937912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>'gg_001_pride_2124.00_September-tenth'</td>\n",
       "      <td>pride</td>\n",
       "      <td>gg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.026206</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.026203</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>-62.777778</td>\n",
       "      <td>-0.002768</td>\n",
       "      <td>...</td>\n",
       "      <td>74.088591</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>11.777778</td>\n",
       "      <td>5.778512e-08</td>\n",
       "      <td>-0.038644</td>\n",
       "      <td>1.761459</td>\n",
       "      <td>599.174643</td>\n",
       "      <td>12.255549</td>\n",
       "      <td>-0.736079</td>\n",
       "      <td>-3.989247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2324 rows × 388 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name     emotion speaker  \\\n",
       "0         'mm_001_happy_2353.51_three-hundred-nine'       happy      mm   \n",
       "1         'mm_001_panic_3395.30_one-thousand-three'       panic      mm   \n",
       "2           'cc_001_panic_861.97_Two-thousand-five'       panic      cc   \n",
       "3          'mf_001_contempt_3901.86_November-first'    contempt      mf   \n",
       "4        'cl_001_interest_1035.82_Ten-thousand-one'    interest      cl   \n",
       "...                                             ...         ...     ...   \n",
       "2319   'gg_001_anxiety_861.72_Three-thousand-eight'     anxiety      gg   \n",
       "2320  'cl_001_cold-anger_499.97_March-twenty-third'  cold-anger      cl   \n",
       "2321      'mm_001_elation_2227.76_nineteen-hundred'     elation      mm   \n",
       "2322        'jg_001_panic_425.32_August-fourteenth'       panic      jg   \n",
       "2323         'gg_001_pride_2124.00_September-tenth'       pride      gg   \n",
       "\n",
       "      frameTime  pcm_RMSenergy_sma_max  pcm_RMSenergy_sma_min  \\\n",
       "0           0.0              -0.036211              -0.000014   \n",
       "1           0.0               0.029315              -0.000044   \n",
       "2           0.0              -0.026964              -0.000068   \n",
       "3           0.0              -0.031901              -0.000035   \n",
       "4           0.0              -0.030345               0.000033   \n",
       "...         ...                    ...                    ...   \n",
       "2319        0.0              -0.022373              -0.000018   \n",
       "2320        0.0               0.063760               0.000017   \n",
       "2321        0.0               0.011188               0.000026   \n",
       "2322        0.0              -0.014303              -0.000023   \n",
       "2323        0.0              -0.026206              -0.000003   \n",
       "\n",
       "      pcm_RMSenergy_sma_range  pcm_RMSenergy_sma_maxPos  \\\n",
       "0                   -0.036197                      78.0   \n",
       "1                    0.029359                      -1.0   \n",
       "2                   -0.026896                       6.0   \n",
       "3                   -0.031866                      52.0   \n",
       "4                   -0.030379                      22.0   \n",
       "...                       ...                       ...   \n",
       "2319                -0.022355                       4.0   \n",
       "2320                 0.063743                      28.0   \n",
       "2321                 0.011161                      20.0   \n",
       "2322                -0.014280                      42.0   \n",
       "2323                -0.026203                     -16.0   \n",
       "\n",
       "      pcm_RMSenergy_sma_minPos  pcm_RMSenergy_sma_amean  ...  F0_sma_de_range  \\\n",
       "0                    72.222222                -0.005396  ...        13.346391   \n",
       "1                    41.222222                 0.003344  ...        77.144891   \n",
       "2                  1760.222222                -0.007719  ...        12.960691   \n",
       "3                   -92.777778                -0.002893  ...       -81.274089   \n",
       "4                    44.222222                -0.005722  ...       -94.339899   \n",
       "...                        ...                      ...  ...              ...   \n",
       "2319                 -2.777778                -0.003906  ...        46.652691   \n",
       "2320                -67.777778                 0.001575  ...       -84.113069   \n",
       "2321                -89.777778                -0.000509  ...        -7.602509   \n",
       "2322                106.222222                -0.006221  ...      -168.599409   \n",
       "2323                -62.777778                -0.002768  ...        74.088591   \n",
       "\n",
       "      F0_sma_de_maxPos  F0_sma_de_minPos  F0_sma_de_amean  F0_sma_de_linregc1  \\\n",
       "0            46.111111         39.777778     9.411793e-08           -0.014334   \n",
       "1            46.111111         15.777778     4.754359e-08           -0.078379   \n",
       "2          1870.111111        564.777778    -1.842239e-10            0.019476   \n",
       "3            23.111111         18.777778     4.950385e-08           -0.009552   \n",
       "4            56.111111         46.777778     2.273961e-08            0.017635   \n",
       "...                ...               ...              ...                 ...   \n",
       "2319        -34.888889        -43.222222     6.623814e-08           -0.008586   \n",
       "2320         39.111111         73.777778    -1.037178e-08            0.015984   \n",
       "2321        -28.888889        -35.222222     1.113053e-08           -0.037956   \n",
       "2322        -38.888889        -52.222222     9.017677e-09            0.019501   \n",
       "2323         21.111111         11.777778     5.778512e-08           -0.038644   \n",
       "\n",
       "      F0_sma_de_linregc2  F0_sma_de_linregerrQ  F0_sma_de_stddev  \\\n",
       "0               1.637743            604.137643         12.327299   \n",
       "1               5.933549           1224.845643         20.930969   \n",
       "2              -1.372460           -245.125257         -5.710581   \n",
       "3               0.117849           -269.904257         -6.556101   \n",
       "4              -1.278651           -418.556177        -13.995534   \n",
       "...                  ...                   ...               ...   \n",
       "2319           -0.017108            199.947643          5.354489   \n",
       "2320           -1.117238           -342.813757         -9.560361   \n",
       "2321            2.413479            416.147443          9.339169   \n",
       "2322           -1.407424           -458.541357        -20.319361   \n",
       "2323            1.761459            599.174643         12.255549   \n",
       "\n",
       "      F0_sma_de_skewness  F0_sma_de_kurtosis  \n",
       "0              -0.478294           -7.692416  \n",
       "1              -0.387068           -7.220758  \n",
       "2              -0.504269            5.277568  \n",
       "3              -0.699956           -5.253465  \n",
       "4              -0.393951           13.258588  \n",
       "...                  ...                 ...  \n",
       "2319           -0.139077            1.651818  \n",
       "2320           -0.405068           -1.692562  \n",
       "2321           -0.271780           -7.857798  \n",
       "2322           -0.387663          -11.937912  \n",
       "2323           -0.736079           -3.989247  \n",
       "\n",
       "[2324 rows x 388 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smile_norm=open_norm(open_smile)\n",
    "smile_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eeed86af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "#Remove one speaker for cross validation\n",
    "cols=smile_norm.columns\n",
    "scores=pd.DataFrame(columns=['speaker', 'accuracy', 'F1', 'n'])\n",
    "i=0\n",
    "reports=[]\n",
    "for speaker in smile_norm['speaker'].unique():\n",
    "    # Filter the data for the current speaker\n",
    "    test = smile_norm[smile_norm['speaker'] == speaker]\n",
    "    # Filter the remaining data for training\n",
    "    train = smile_norm[smile_norm['speaker'] != speaker]\n",
    "    features= list(train.columns[4:])\n",
    "\n",
    "    X_train= train[features]\n",
    "    y_train= train['emotion']\n",
    "\n",
    "    X_test= test[features]\n",
    "    y_test= test['emotion']\n",
    "    \n",
    "    #only OpenSmile\n",
    "    rf_classifier = RandomForestClassifier(random_state=42, n_estimators=100, verbose=0)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    n=test.shape[0]\n",
    "    \n",
    "    row=[speaker, accuracy, f1, n]\n",
    "    scores.loc[i]=row\n",
    "    i+=1\n",
    "    reports.append(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d0eb56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)\n",
    "len(set(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27f0e93c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mm</td>\n",
       "      <td>0.261589</td>\n",
       "      <td>0.244635</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.216585</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mf</td>\n",
       "      <td>0.170569</td>\n",
       "      <td>0.153532</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cl</td>\n",
       "      <td>0.184783</td>\n",
       "      <td>0.167348</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mk</td>\n",
       "      <td>0.156171</td>\n",
       "      <td>0.129367</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jg</td>\n",
       "      <td>0.150183</td>\n",
       "      <td>0.140944</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gg</td>\n",
       "      <td>0.235714</td>\n",
       "      <td>0.193932</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker  accuracy        F1    n\n",
       "0      mm  0.261589  0.244635  302\n",
       "1      cc  0.207547  0.216585  265\n",
       "2      mf  0.170569  0.153532  299\n",
       "3      cl  0.184783  0.167348  368\n",
       "4      mk  0.156171  0.129367  397\n",
       "5      jg  0.150183  0.140944  273\n",
       "6      gg  0.235714  0.193932  420"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "555bf921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Average Accuracry:  0.19578313253012047\n",
      "Aggreaged F1:  0.17644250308557874\n"
     ]
    }
   ],
   "source": [
    "#RF Aggregated Averages\n",
    "\n",
    "assert(scores['n'].sum()==2324)\n",
    "n_total= scores['n'].sum()\n",
    "accuracy=0\n",
    "F1=0\n",
    "#will run 7 times\n",
    "for index, row in scores.iterrows():\n",
    "    speaker= row['speaker']\n",
    "    acc= row['accuracy']\n",
    "    f1= row['F1']\n",
    "    n= row['n']\n",
    "    accuracy+= acc * n\n",
    "    F1+= f1 * n\n",
    "\n",
    "aggregated_accuracy= accuracy/n_total\n",
    "aggregated_F1= F1/n_total\n",
    "\n",
    "print(\"Aggregated Average Accuracry: \", aggregated_accuracy)\n",
    "print(\"Aggreaged F1: \", aggregated_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe8dece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pre><code>          precision    recall  f1-score   support\n",
      "\n",
      " anxiety       0.57      0.33      0.42        39\n",
      " boredom       0.12      0.05      0.07        19\n",
      "</code></pre>\n",
      "<p>cold-anger       0.10      0.05      0.07        20\n",
      "    contempt       0.10      0.05      0.07        19\n",
      "     despair       0.14      0.11      0.12        18\n",
      "     disgust       0.29      0.26      0.27        23\n",
      "     elation       0.31      0.21      0.25        19\n",
      "       happy       0.39      0.50      0.44        18\n",
      "   hot-anger       0.40      0.38      0.39        16\n",
      "    interest       0.24      0.48      0.32        21\n",
      "     neutral       0.00      0.00      0.00         9\n",
      "       panic       0.44      0.14      0.22        28\n",
      "       pride       0.27      0.21      0.24        19\n",
      "     sadness       0.12      0.29      0.17        17\n",
      "       shame       0.23      0.76      0.35        17</p>\n",
      "<pre><code>accuracy                           0.26       302\n",
      "</code></pre>\n",
      "<p>macro avg       0.25      0.26      0.23       302\n",
      "weighted avg       0.28      0.26      0.24       302</p>\n",
      "<pre><code>          precision    recall  f1-score   support\n",
      "\n",
      " anxiety       0.04      0.30      0.08        10\n",
      " boredom       0.17      0.07      0.10        15\n",
      "</code></pre>\n",
      "<p>cold-anger       0.04      0.07      0.05        15\n",
      "    contempt       0.21      0.27      0.24        22\n",
      "     despair       0.00      0.00      0.00         9\n",
      "     disgust       0.25      0.03      0.06        31\n",
      "     elation       0.08      0.12      0.10        16\n",
      "       happy       0.24      0.17      0.20        23\n",
      "   hot-anger       0.53      0.64      0.58        14\n",
      "    interest       0.30      0.18      0.22        17\n",
      "     neutral       0.67      0.44      0.53        18\n",
      "       panic       0.50      0.44      0.47        18\n",
      "       pride       0.22      0.09      0.12        23\n",
      "     sadness       0.20      0.15      0.17        13\n",
      "       shame       0.38      0.24      0.29        21</p>\n",
      "<pre><code>accuracy                           0.21       265\n",
      "</code></pre>\n",
      "<p>macro avg       0.26      0.21      0.21       265\n",
      "weighted avg       0.27      0.21      0.22       265</p>\n",
      "<pre><code>          precision    recall  f1-score   support\n",
      "\n",
      " anxiety       0.17      0.41      0.24        22\n",
      " boredom       0.19      0.30      0.23        27\n",
      "</code></pre>\n",
      "<p>cold-anger       0.00      0.00      0.00        20\n",
      "    contempt       0.50      0.09      0.15        44\n",
      "     despair       0.00      0.00      0.00        16\n",
      "     disgust       0.00      0.00      0.00         1\n",
      "     elation       0.12      0.08      0.09        26\n",
      "       happy       0.05      0.09      0.07        23\n",
      "   hot-anger       0.20      0.10      0.13        21\n",
      "    interest       0.26      0.37      0.30        19\n",
      "     neutral       0.30      0.30      0.30        10\n",
      "       panic       0.25      0.50      0.33        12\n",
      "       pride       0.14      0.17      0.15        18\n",
      "     sadness       0.00      0.00      0.00        20\n",
      "       shame       0.29      0.25      0.27        20</p>\n",
      "<pre><code>accuracy                           0.17       299\n",
      "</code></pre>\n",
      "<p>macro avg       0.16      0.18      0.15       299\n",
      "weighted avg       0.20      0.17      0.15       299</p>\n",
      "<pre><code>          precision    recall  f1-score   support\n",
      "\n",
      " anxiety       0.14      0.43      0.21        21\n",
      " boredom       0.30      0.83      0.44        29\n",
      "</code></pre>\n",
      "<p>cold-anger       0.44      0.15      0.22        27\n",
      "    contempt       0.12      0.08      0.10        25\n",
      "     despair       0.20      0.10      0.14        29\n",
      "     disgust       0.00      0.00      0.00        22\n",
      "     elation       0.75      0.22      0.34        27\n",
      "       happy       0.60      0.14      0.23        21\n",
      "   hot-anger       0.24      0.15      0.19        26\n",
      "    interest       0.12      0.04      0.06        26\n",
      "     neutral       0.00      0.00      0.00        17\n",
      "       panic       0.00      0.00      0.00        21\n",
      "       pride       0.50      0.17      0.25        24\n",
      "     sadness       0.13      0.26      0.17        27\n",
      "       shame       0.02      0.04      0.03        26</p>\n",
      "<pre><code>accuracy                           0.18       368\n",
      "</code></pre>\n",
      "<p>macro avg       0.24      0.17      0.16       368\n",
      "weighted avg       0.25      0.18      0.17       368</p>\n",
      "<pre><code>          precision    recall  f1-score   support\n",
      "\n",
      " anxiety       0.17      0.07      0.10        29\n",
      " boredom       0.07      0.05      0.06        20\n",
      "</code></pre>\n",
      "<p>cold-anger       0.13      0.26      0.17        23\n",
      "    contempt       0.13      0.29      0.18        21\n",
      "     despair       0.00      0.00      0.00        53\n",
      "     disgust       0.05      0.05      0.05        21\n",
      "     elation       0.11      0.30      0.16        23\n",
      "       happy       0.22      0.31      0.25        42\n",
      "   hot-anger       0.26      0.59      0.36        22\n",
      "    interest       0.13      0.11      0.12        44\n",
      "     neutral       0.00      0.00      0.00         8\n",
      "       panic       0.36      0.19      0.25        21\n",
      "       pride       0.00      0.00      0.00        23\n",
      "     sadness       0.00      0.00      0.00        22\n",
      "       shame       0.40      0.16      0.23        25</p>\n",
      "<pre><code>accuracy                           0.16       397\n",
      "</code></pre>\n",
      "<p>macro avg       0.13      0.16      0.13       397\n",
      "weighted avg       0.13      0.16      0.13       397</p>\n",
      "<pre><code>          precision    recall  f1-score   support\n",
      "\n",
      " anxiety       0.14      0.16      0.15        19\n",
      " boredom       0.12      0.07      0.09        14\n",
      "</code></pre>\n",
      "<p>cold-anger       0.11      0.18      0.14        22\n",
      "    contempt       0.06      0.09      0.07        23\n",
      "     despair       0.17      0.24      0.20        21\n",
      "     disgust       0.21      0.30      0.25        23\n",
      "     elation       0.31      0.20      0.24        20\n",
      "       happy       0.21      0.20      0.21        20\n",
      "   hot-anger       0.33      0.33      0.33        18\n",
      "    interest       0.13      0.21      0.16        19\n",
      "     neutral       0.00      0.00      0.00         8\n",
      "       panic       0.00      0.00      0.00        14\n",
      "       pride       0.00      0.00      0.00        18\n",
      "     sadness       0.50      0.05      0.10        19\n",
      "       shame       0.00      0.00      0.00        15</p>\n",
      "<pre><code>accuracy                           0.15       273\n",
      "</code></pre>\n",
      "<p>macro avg       0.15      0.14      0.13       273\n",
      "weighted avg       0.16      0.15      0.14       273</p>\n",
      "<pre><code>          precision    recall  f1-score   support\n",
      "\n",
      " anxiety       0.38      0.37      0.37        30\n",
      " boredom       0.17      0.03      0.06        30\n",
      "</code></pre>\n",
      "<p>cold-anger       0.24      0.33      0.28        27\n",
      "    contempt       0.30      0.31      0.30        26\n",
      "     despair       0.00      0.00      0.00        28\n",
      "     disgust       0.22      0.04      0.07        51\n",
      "     elation       0.23      0.57      0.33        28\n",
      "       happy       0.14      0.30      0.19        30\n",
      "   hot-anger       0.25      0.59      0.35        22\n",
      "    interest       0.13      0.13      0.13        30\n",
      "     neutral       1.00      0.11      0.20         9\n",
      "       panic       0.36      0.63      0.46        27\n",
      "       pride       0.12      0.12      0.12        25\n",
      "     sadness       0.00      0.00      0.00        33\n",
      "       shame       0.31      0.21      0.25        24</p>\n",
      "<pre><code>accuracy                           0.24       420\n",
      "</code></pre>\n",
      "<p>macro avg       0.26      0.25      0.21       420\n",
      "weighted avg       0.22      0.24      0.19       420</p>\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "for report in reports:\n",
    "    print(markdown.markdown(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ad663985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th iteration done\n",
      "1th iteration done\n",
      "2th iteration done\n",
      "3th iteration done\n",
      "4th iteration done\n",
      "5th iteration done\n",
      "6th iteration done\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "#Remove one speaker for cross validation\n",
    "cols=smile_norm.columns\n",
    "scores_svm=pd.DataFrame(columns=['speaker', 'accuracy', 'F1', 'n'])\n",
    "i=0\n",
    "for speaker in smile_norm['speaker'].unique():\n",
    "    # Filter the data for the current speaker\n",
    "    test = smile_norm[smile_norm['speaker'] == speaker]\n",
    "    # Filter the remaining data for training\n",
    "    train = smile_norm[smile_norm['speaker'] != speaker]\n",
    "    features= list(train.columns[4:])\n",
    "\n",
    "    X_train= train[features]\n",
    "    y_train= train['emotion']\n",
    "\n",
    "    X_test= test[features]\n",
    "    y_test= test['emotion']\n",
    "    \n",
    "    #only OpenSmile\n",
    "    svm_classifier = svm.SVC(kernel='rbf', C=10, gamma='scale', random_state=43)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    print(f\"{i}th iteration done\")\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    n=test.shape[0]\n",
    "    \n",
    "    row=[speaker, accuracy, f1, n]\n",
    "    scores_svm.loc[i]=row\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "04724bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mm</td>\n",
       "      <td>0.135762</td>\n",
       "      <td>0.077793</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.058617</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mf</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.065504</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cl</td>\n",
       "      <td>0.078804</td>\n",
       "      <td>0.031958</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mk</td>\n",
       "      <td>0.100756</td>\n",
       "      <td>0.060938</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jg</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>0.087482</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gg</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>0.057588</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker  accuracy        F1    n\n",
       "0      mm  0.135762  0.077793  302\n",
       "1      cc  0.094340  0.058617  265\n",
       "2      mf  0.130435  0.065504  299\n",
       "3      cl  0.078804  0.031958  368\n",
       "4      mk  0.100756  0.060938  397\n",
       "5      jg  0.109890  0.087482  273\n",
       "6      gg  0.109524  0.057588  420"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d9fe7d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Average Accuracry:  0.10757314974182444\n",
      "Aggreaged F1:  0.061374998854800786\n"
     ]
    }
   ],
   "source": [
    "#SVM Aggregated Averages\n",
    "\n",
    "assert(scores_svm['n'].sum()==2324)\n",
    "n_total= scores_svm['n'].sum()\n",
    "accuracy=0\n",
    "F1=0\n",
    "#will run 7 times\n",
    "for index, row in scores_svm.iterrows():\n",
    "    speaker= row['speaker']\n",
    "    acc= row['accuracy']\n",
    "    f1= row['F1']\n",
    "    n= row['n']\n",
    "    accuracy+= acc * n\n",
    "    F1+= f1 * n\n",
    "\n",
    "aggregated_accuracy= accuracy/n_total\n",
    "aggregated_F1= F1/n_total\n",
    "\n",
    "print(\"Aggregated Average Accuracry: \", aggregated_accuracy)\n",
    "print(\"Aggreaged F1: \", aggregated_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6c090417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before scaling:\n",
      "X_train: (2022, 384)\n",
      "X_test: (302, 384)\n",
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 7ms/step - loss: 3.6416 - accuracy: 0.0826\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 3.0846 - accuracy: 0.1202\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.7976 - accuracy: 0.1667\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.5942 - accuracy: 0.1919\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.4373 - accuracy: 0.2191\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.3084 - accuracy: 0.2527\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.2056 - accuracy: 0.2760\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1209 - accuracy: 0.3056\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.0392 - accuracy: 0.3249\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.9714 - accuracy: 0.3526\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.9087 - accuracy: 0.3729\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8549 - accuracy: 0.3892\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8053 - accuracy: 0.4105\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7596 - accuracy: 0.4258\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7192 - accuracy: 0.4416\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6822 - accuracy: 0.4525\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6469 - accuracy: 0.4723\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6155 - accuracy: 0.4797\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5857 - accuracy: 0.4916\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5570 - accuracy: 0.5005\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5311 - accuracy: 0.5079\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5066 - accuracy: 0.5237\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4819 - accuracy: 0.5252\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4609 - accuracy: 0.5391\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4389 - accuracy: 0.5430\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4200 - accuracy: 0.5524\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4001 - accuracy: 0.5584\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3820 - accuracy: 0.5579\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3631 - accuracy: 0.5712\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3459 - accuracy: 0.5712\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3311 - accuracy: 0.5737\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3145 - accuracy: 0.5776\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3001 - accuracy: 0.5846\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2842 - accuracy: 0.5910\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2716 - accuracy: 0.5915\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2571 - accuracy: 0.5964\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2437 - accuracy: 0.5999\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2321 - accuracy: 0.6053\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2199 - accuracy: 0.6053\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2092 - accuracy: 0.6113\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1979 - accuracy: 0.6108\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1854 - accuracy: 0.6187\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1762 - accuracy: 0.6162\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1630 - accuracy: 0.6231\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1556 - accuracy: 0.6291\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1443 - accuracy: 0.6296\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1346 - accuracy: 0.6320\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1225 - accuracy: 0.6360\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1154 - accuracy: 0.6330\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1068 - accuracy: 0.6390\n",
      "0th iteration done\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "Validation F1 Score (macro): 0.22751326287332121\n",
      "Validation Accuracy: 0.25496688741721857\n",
      "Shapes before scaling:\n",
      "X_train: (2059, 384)\n",
      "X_test: (265, 384)\n",
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 7ms/step - loss: 3.3950 - accuracy: 0.1030\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.9426 - accuracy: 0.1442\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.6696 - accuracy: 0.1841\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.4728 - accuracy: 0.2195\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.3210 - accuracy: 0.2555\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.2012 - accuracy: 0.2885\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0987 - accuracy: 0.3205\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0138 - accuracy: 0.3409\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9399 - accuracy: 0.3711\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8758 - accuracy: 0.3944\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8171 - accuracy: 0.4119\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7654 - accuracy: 0.4293\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7174 - accuracy: 0.4492\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6738 - accuracy: 0.4648\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6375 - accuracy: 0.4847\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5993 - accuracy: 0.4930\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5666 - accuracy: 0.5051\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.5337 - accuracy: 0.5187\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5057 - accuracy: 0.5279\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4769 - accuracy: 0.5313\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4512 - accuracy: 0.5386\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4275 - accuracy: 0.5503\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4031 - accuracy: 0.5561\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3810 - accuracy: 0.5648\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3574 - accuracy: 0.5712\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3391 - accuracy: 0.5750\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3198 - accuracy: 0.5784\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3011 - accuracy: 0.5862\n",
      "Epoch 29/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2851 - accuracy: 0.5940\n",
      "Epoch 30/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.2664 - accuracy: 0.5974\n",
      "Epoch 31/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2500 - accuracy: 0.6061\n",
      "Epoch 32/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.2355 - accuracy: 0.6090\n",
      "Epoch 33/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.2199 - accuracy: 0.6173\n",
      "Epoch 34/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.2047 - accuracy: 0.6212\n",
      "Epoch 35/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1935 - accuracy: 0.6275\n",
      "Epoch 36/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1792 - accuracy: 0.6275\n",
      "Epoch 37/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1655 - accuracy: 0.6377\n",
      "Epoch 38/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1528 - accuracy: 0.6343\n",
      "Epoch 39/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1406 - accuracy: 0.6425\n",
      "Epoch 40/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1285 - accuracy: 0.6484\n",
      "Epoch 41/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1177 - accuracy: 0.6523\n",
      "Epoch 42/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1069 - accuracy: 0.6591\n",
      "Epoch 43/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0993 - accuracy: 0.6581\n",
      "Epoch 44/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0874 - accuracy: 0.6629\n",
      "Epoch 45/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0767 - accuracy: 0.6712\n",
      "Epoch 46/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.0665 - accuracy: 0.6649\n",
      "Epoch 47/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.0556 - accuracy: 0.6770\n",
      "Epoch 48/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.0488 - accuracy: 0.6785\n",
      "Epoch 49/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0396 - accuracy: 0.6785\n",
      "Epoch 50/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0283 - accuracy: 0.6853\n",
      "1th iteration done\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "Validation F1 Score (macro): 0.19002006463188012\n",
      "Validation Accuracy: 0.20754716981132076\n",
      "Shapes before scaling:\n",
      "X_train: (2025, 384)\n",
      "X_test: (299, 384)\n",
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 6ms/step - loss: 3.6158 - accuracy: 0.0874\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 3.0720 - accuracy: 0.1338\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.7840 - accuracy: 0.1654\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.5779 - accuracy: 0.2035\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 2.4187 - accuracy: 0.2336\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 2.2927 - accuracy: 0.2677\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 2.1873 - accuracy: 0.2948\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1003 - accuracy: 0.3244\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 2.0206 - accuracy: 0.3447\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9514 - accuracy: 0.3674\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8888 - accuracy: 0.3832\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8337 - accuracy: 0.4015\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.7830 - accuracy: 0.4291\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.7395 - accuracy: 0.4454\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.6955 - accuracy: 0.4588\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6572 - accuracy: 0.4677\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.6214 - accuracy: 0.4825\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.5874 - accuracy: 0.4904\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.5575 - accuracy: 0.5062\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.5278 - accuracy: 0.5126\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5010 - accuracy: 0.5230\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4753 - accuracy: 0.5353\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4496 - accuracy: 0.5383\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4270 - accuracy: 0.5526\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4028 - accuracy: 0.5635\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3854 - accuracy: 0.5635\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3635 - accuracy: 0.5684\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3437 - accuracy: 0.5753\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3262 - accuracy: 0.5778\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3088 - accuracy: 0.5832\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2900 - accuracy: 0.5877\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2747 - accuracy: 0.5960\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2600 - accuracy: 0.5985\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2433 - accuracy: 0.6049\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2309 - accuracy: 0.6099\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2179 - accuracy: 0.6099\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2038 - accuracy: 0.6148\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1871 - accuracy: 0.6198\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1761 - accuracy: 0.6247\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1656 - accuracy: 0.6277\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1530 - accuracy: 0.6331\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1428 - accuracy: 0.6380\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1290 - accuracy: 0.6341\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1220 - accuracy: 0.6430\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1086 - accuracy: 0.6479\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.1005 - accuracy: 0.6543\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0901 - accuracy: 0.6533\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0817 - accuracy: 0.6558\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0699 - accuracy: 0.6588\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.0615 - accuracy: 0.6568\n",
      "2th iteration done\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "Validation F1 Score (macro): 0.18852465004627073\n",
      "Validation Accuracy: 0.20735785953177258\n",
      "Shapes before scaling:\n",
      "X_train: (1956, 384)\n",
      "X_test: (368, 384)\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 1s 6ms/step - loss: 3.5166 - accuracy: 0.0976\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 3.0515 - accuracy: 0.1360\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.7675 - accuracy: 0.1743\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.5596 - accuracy: 0.2111\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.3989 - accuracy: 0.2505\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.2764 - accuracy: 0.2781\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.1765 - accuracy: 0.3011\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 2.0920 - accuracy: 0.3303\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 2.0172 - accuracy: 0.3533\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.9536 - accuracy: 0.3742\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.8962 - accuracy: 0.3926\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.8464 - accuracy: 0.4090\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.7987 - accuracy: 0.4279\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.7555 - accuracy: 0.4371\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.7177 - accuracy: 0.4504\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.6820 - accuracy: 0.4550\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.6482 - accuracy: 0.4729\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.6179 - accuracy: 0.4867\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5872 - accuracy: 0.4964\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.5588 - accuracy: 0.5072\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.5353 - accuracy: 0.5143\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.5115 - accuracy: 0.5256\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.4900 - accuracy: 0.5276\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.4670 - accuracy: 0.5348\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.4459 - accuracy: 0.5419\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.4262 - accuracy: 0.5521\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4061 - accuracy: 0.5583\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3904 - accuracy: 0.5660\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3723 - accuracy: 0.5700\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3555 - accuracy: 0.5803\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3394 - accuracy: 0.5874\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3219 - accuracy: 0.5869\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.3085 - accuracy: 0.5956\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2950 - accuracy: 0.5982\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2803 - accuracy: 0.6033\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.2672 - accuracy: 0.6094\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2521 - accuracy: 0.6135\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2411 - accuracy: 0.6145\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2267 - accuracy: 0.6196\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2157 - accuracy: 0.6212\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.2051 - accuracy: 0.6263\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1937 - accuracy: 0.6329\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1812 - accuracy: 0.6370\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.1719 - accuracy: 0.6375\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1613 - accuracy: 0.6380\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1503 - accuracy: 0.6457\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1413 - accuracy: 0.6467\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1319 - accuracy: 0.6539\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1204 - accuracy: 0.6585\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.1111 - accuracy: 0.6646\n",
      "3th iteration done\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "Validation F1 Score (macro): 0.18265877747907133\n",
      "Validation Accuracy: 0.17119565217391305\n",
      "Shapes before scaling:\n",
      "X_train: (1927, 384)\n",
      "X_test: (397, 384)\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 1s 7ms/step - loss: 3.5149 - accuracy: 0.0898\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.0118 - accuracy: 0.1370\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.7333 - accuracy: 0.1655\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.5304 - accuracy: 0.1915\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.3692 - accuracy: 0.2206\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.2422 - accuracy: 0.2610\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 2.1352 - accuracy: 0.3046\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.0449 - accuracy: 0.3285\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.9692 - accuracy: 0.3550\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.9012 - accuracy: 0.3783\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.8421 - accuracy: 0.4006\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.7883 - accuracy: 0.4224\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.7411 - accuracy: 0.4375\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.6980 - accuracy: 0.4510\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.6561 - accuracy: 0.4738\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.6183 - accuracy: 0.4800\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.5839 - accuracy: 0.4925\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.5521 - accuracy: 0.5065\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.5215 - accuracy: 0.5179\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.4913 - accuracy: 0.5278\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.4659 - accuracy: 0.5314\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.4400 - accuracy: 0.5454\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.4162 - accuracy: 0.5547\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.3920 - accuracy: 0.5568\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.3697 - accuracy: 0.5672\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.3489 - accuracy: 0.5765\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.3314 - accuracy: 0.5802\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.3090 - accuracy: 0.5864\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.2915 - accuracy: 0.5916\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.2745 - accuracy: 0.5963\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.2563 - accuracy: 0.6035\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.2403 - accuracy: 0.6103\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.2252 - accuracy: 0.6144\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.2095 - accuracy: 0.6243\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.1948 - accuracy: 0.6243\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.1808 - accuracy: 0.6373\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1674 - accuracy: 0.6357\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1547 - accuracy: 0.6424\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1436 - accuracy: 0.6502\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1296 - accuracy: 0.6544\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1191 - accuracy: 0.6544\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1074 - accuracy: 0.6611\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.0955 - accuracy: 0.6694\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.0861 - accuracy: 0.6741\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.0742 - accuracy: 0.6788\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.0649 - accuracy: 0.6834\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.0543 - accuracy: 0.6814\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.0429 - accuracy: 0.6866\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.0344 - accuracy: 0.6902\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.0266 - accuracy: 0.6917\n",
      "4th iteration done\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "Validation F1 Score (macro): 0.18040667518578563\n",
      "Validation Accuracy: 0.1964735516372796\n",
      "Shapes before scaling:\n",
      "X_train: (2051, 384)\n",
      "X_test: (273, 384)\n",
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 6ms/step - loss: 3.7226 - accuracy: 0.0843\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 3.0839 - accuracy: 0.1394\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 2.7656 - accuracy: 0.1872\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.5571 - accuracy: 0.2194\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.4031 - accuracy: 0.2511\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.2825 - accuracy: 0.2755\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 2.1783 - accuracy: 0.3077\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0902 - accuracy: 0.3306\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 2.0140 - accuracy: 0.3530\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.9429 - accuracy: 0.3793\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.8848 - accuracy: 0.3920\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8295 - accuracy: 0.4110\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.7809 - accuracy: 0.4320\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.7356 - accuracy: 0.4495\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.6949 - accuracy: 0.4608\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.6560 - accuracy: 0.4764\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6207 - accuracy: 0.4939\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.5865 - accuracy: 0.4978\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.5581 - accuracy: 0.5095\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.5286 - accuracy: 0.5178\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.5012 - accuracy: 0.5290\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.4764 - accuracy: 0.5378\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.4516 - accuracy: 0.5480\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.4290 - accuracy: 0.5480\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.4090 - accuracy: 0.5583\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.3862 - accuracy: 0.5714\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.3662 - accuracy: 0.5734\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3482 - accuracy: 0.5807\n",
      "Epoch 29/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.3300 - accuracy: 0.5846\n",
      "Epoch 30/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.3133 - accuracy: 0.5963\n",
      "Epoch 31/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.2949 - accuracy: 0.5968\n",
      "Epoch 32/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.2803 - accuracy: 0.5987\n",
      "Epoch 33/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2641 - accuracy: 0.6046\n",
      "Epoch 34/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2508 - accuracy: 0.6138\n",
      "Epoch 35/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.2350 - accuracy: 0.6221\n",
      "Epoch 36/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2224 - accuracy: 0.6163\n",
      "Epoch 37/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2079 - accuracy: 0.6280\n",
      "Epoch 38/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1956 - accuracy: 0.6246\n",
      "Epoch 39/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1819 - accuracy: 0.6333\n",
      "Epoch 40/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1710 - accuracy: 0.6329\n",
      "Epoch 41/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1604 - accuracy: 0.6416\n",
      "Epoch 42/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1491 - accuracy: 0.6421\n",
      "Epoch 43/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1375 - accuracy: 0.6499\n",
      "Epoch 44/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1269 - accuracy: 0.6509\n",
      "Epoch 45/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1190 - accuracy: 0.6494\n",
      "Epoch 46/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.1082 - accuracy: 0.6538\n",
      "Epoch 47/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0971 - accuracy: 0.6563\n",
      "Epoch 48/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0880 - accuracy: 0.6626\n",
      "Epoch 49/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0794 - accuracy: 0.6665\n",
      "Epoch 50/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0742 - accuracy: 0.6665\n",
      "5th iteration done\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "Validation F1 Score (macro): 0.14283870345062163\n",
      "Validation Accuracy: 0.14652014652014653\n",
      "Shapes before scaling:\n",
      "X_train: (1904, 384)\n",
      "X_test: (420, 384)\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 1s 6ms/step - loss: 3.3891 - accuracy: 0.0903\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.9282 - accuracy: 0.1392\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.6829 - accuracy: 0.1759\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.5001 - accuracy: 0.2090\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.3631 - accuracy: 0.2379\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.2513 - accuracy: 0.2605\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1581 - accuracy: 0.2878\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0764 - accuracy: 0.3157\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0092 - accuracy: 0.3372\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9466 - accuracy: 0.3603\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8964 - accuracy: 0.3750\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8475 - accuracy: 0.3965\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8017 - accuracy: 0.4112\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7615 - accuracy: 0.4207\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7243 - accuracy: 0.4343\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6891 - accuracy: 0.4496\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6572 - accuracy: 0.4506\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6269 - accuracy: 0.4643\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.5975 - accuracy: 0.4732\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5684 - accuracy: 0.4863\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5434 - accuracy: 0.4942\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5196 - accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4965 - accuracy: 0.5079\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4732 - accuracy: 0.5242\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4545 - accuracy: 0.5284\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4320 - accuracy: 0.5362\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4140 - accuracy: 0.5441\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3950 - accuracy: 0.5436\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3755 - accuracy: 0.5530\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3596 - accuracy: 0.5536\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3416 - accuracy: 0.5683\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3267 - accuracy: 0.5672\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.3102 - accuracy: 0.5783\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.2957 - accuracy: 0.5793\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2803 - accuracy: 0.5872\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2677 - accuracy: 0.5898\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2526 - accuracy: 0.5977\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2415 - accuracy: 0.6014\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2303 - accuracy: 0.6045\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2151 - accuracy: 0.6092\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.2033 - accuracy: 0.6192\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1920 - accuracy: 0.6213\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1798 - accuracy: 0.6261\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.1699 - accuracy: 0.6324\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1588 - accuracy: 0.6318\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1505 - accuracy: 0.6339\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1373 - accuracy: 0.6371\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.1285 - accuracy: 0.6397\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1182 - accuracy: 0.6408\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.1098 - accuracy: 0.6455\n",
      "6th iteration done\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Validation F1 Score (macro): 0.24643831386337625\n",
      "Validation Accuracy: 0.2761904761904762\n"
     ]
    }
   ],
   "source": [
    "#try neural nets\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def build_mlp_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#Remove one speaker for cross validation\n",
    "cols=smile_norm.columns\n",
    "scores_nn=pd.DataFrame(columns=['speaker', 'accuracy', 'F1', 'n'])\n",
    "i=0\n",
    "for speaker in smile_norm['speaker'].unique():\n",
    "    # Filter the data for the current speaker\n",
    "    test = smile_norm[smile_norm['speaker'] == speaker]\n",
    "    # Filter the remaining data for training\n",
    "    train = smile_norm[smile_norm['speaker'] != speaker]\n",
    "    features= list(train.columns[4:])\n",
    "\n",
    "    X_train= train[features]\n",
    "    y_train= train['emotion']\n",
    "\n",
    "    X_test= test[features]\n",
    "    y_test= test['emotion']\n",
    "    \n",
    "    #only OpenSmile\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "    \n",
    "    print(\"Shapes before scaling:\")\n",
    "    print(\"X_train:\", X_train.shape)\n",
    "    print(\"X_test:\", X_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = build_mlp_model(input_shape=X_train.shape[1], num_classes=num_classes)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "    print(f\"{i}th iteration done\")\n",
    "    y_val_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_val_pred)\n",
    "    f1 = f1_score(y_test, y_val_pred, average='weighted')\n",
    "\n",
    "    print(\"Validation F1 Score (macro):\", f1)\n",
    "    print(\"Validation Accuracy:\", accuracy)\n",
    "    n=test.shape[0]\n",
    "    \n",
    "    row=[speaker, accuracy, f1, n]\n",
    "    scores_nn.loc[i]=row\n",
    "    i+=1\n",
    "    if f1 < 0.20 and i < 1: \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a1200b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mm</td>\n",
       "      <td>0.254967</td>\n",
       "      <td>0.227513</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.190020</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mf</td>\n",
       "      <td>0.207358</td>\n",
       "      <td>0.188525</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cl</td>\n",
       "      <td>0.171196</td>\n",
       "      <td>0.182659</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mk</td>\n",
       "      <td>0.196474</td>\n",
       "      <td>0.180407</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jg</td>\n",
       "      <td>0.146520</td>\n",
       "      <td>0.142839</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gg</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>0.246438</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker  accuracy        F1    n\n",
       "0      mm  0.254967  0.227513  302\n",
       "1      cc  0.207547  0.190020  265\n",
       "2      mf  0.207358  0.188525  299\n",
       "3      cl  0.171196  0.182659  368\n",
       "4      mk  0.196474  0.180407  397\n",
       "5      jg  0.146520  0.142839  273\n",
       "6      gg  0.276190  0.246438  420"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6b7f033a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Average Accuracry:  0.2112736660929432\n",
      "Aggreaged F1:  0.1965456673428223\n"
     ]
    }
   ],
   "source": [
    "assert(scores_nn['n'].sum()==2324)\n",
    "n_total= scores_nn['n'].sum()\n",
    "accuracy=0\n",
    "F1=0\n",
    "#will run 7 times\n",
    "for index, row in scores_nn.iterrows():\n",
    "    speaker= row['speaker']\n",
    "    acc= row['accuracy']\n",
    "    f1= row['F1']\n",
    "    n= row['n']\n",
    "    accuracy+= acc * n\n",
    "    F1+= f1 * n\n",
    "\n",
    "aggregated_accuracy= accuracy/n_total\n",
    "aggregated_F1= F1/n_total\n",
    "\n",
    "print(\"Aggregated Average Accuracry: \", aggregated_accuracy)\n",
    "print(\"Aggreaged F1: \", aggregated_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7e345bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before scaling:\n",
      "X_train: (2022, 384)\n",
      "X_test: (302, 384)\n",
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 7ms/step - loss: 2.7420 - accuracy: 0.0959\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.6235 - accuracy: 0.1454\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.5626 - accuracy: 0.1993\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.5087 - accuracy: 0.2384\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.4610 - accuracy: 0.2582\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.4160 - accuracy: 0.2819\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.3740 - accuracy: 0.2868\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.3342 - accuracy: 0.3081\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.2985 - accuracy: 0.3195\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.2629 - accuracy: 0.3383\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.2291 - accuracy: 0.3640\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1955 - accuracy: 0.3813\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1635 - accuracy: 0.3853\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1338 - accuracy: 0.4031\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1014 - accuracy: 0.4164\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.0714 - accuracy: 0.4228\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.0424 - accuracy: 0.4342\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.0129 - accuracy: 0.4476\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9853 - accuracy: 0.4550\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9581 - accuracy: 0.4708\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9303 - accuracy: 0.4753\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9036 - accuracy: 0.4862\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8790 - accuracy: 0.4906\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8530 - accuracy: 0.5035\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8267 - accuracy: 0.5119\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8021 - accuracy: 0.5218\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7785 - accuracy: 0.5317\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7546 - accuracy: 0.5381\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7317 - accuracy: 0.5480\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7089 - accuracy: 0.5559\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6860 - accuracy: 0.5593\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6639 - accuracy: 0.5584\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6432 - accuracy: 0.5678\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6218 - accuracy: 0.5712\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6010 - accuracy: 0.5722\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5818 - accuracy: 0.5846\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5610 - accuracy: 0.5900\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5427 - accuracy: 0.5875\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5229 - accuracy: 0.5950\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5040 - accuracy: 0.6044\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4862 - accuracy: 0.6108\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4683 - accuracy: 0.6147\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4508 - accuracy: 0.6227\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4330 - accuracy: 0.6177\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4164 - accuracy: 0.6286\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3998 - accuracy: 0.6335\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3827 - accuracy: 0.6370\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3668 - accuracy: 0.6434\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3516 - accuracy: 0.6454\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3355 - accuracy: 0.6513\n",
      "0th iteration done\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.3009653306555591\n",
      "Validation Accuracy: 0.31456953642384106\n",
      "Shapes before scaling:\n",
      "X_train: (2059, 384)\n",
      "X_test: (265, 384)\n",
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 7ms/step - loss: 2.7729 - accuracy: 0.0855\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.6152 - accuracy: 0.1452\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.5507 - accuracy: 0.2074\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.4974 - accuracy: 0.2618\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.4482 - accuracy: 0.2938\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.4025 - accuracy: 0.2880\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.3598 - accuracy: 0.3249\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.3208 - accuracy: 0.3356\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.2809 - accuracy: 0.3560\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.2437 - accuracy: 0.3803\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.2083 - accuracy: 0.3910\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.1735 - accuracy: 0.4012\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.1391 - accuracy: 0.4177\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.1057 - accuracy: 0.4269\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0724 - accuracy: 0.4288\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0410 - accuracy: 0.4458\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0084 - accuracy: 0.4570\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9765 - accuracy: 0.4677\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9470 - accuracy: 0.4745\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9180 - accuracy: 0.4915\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8884 - accuracy: 0.4886\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8592 - accuracy: 0.5032\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8318 - accuracy: 0.5109\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8041 - accuracy: 0.5221\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7786 - accuracy: 0.5270\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7530 - accuracy: 0.5396\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7268 - accuracy: 0.5454\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7013 - accuracy: 0.5566\n",
      "Epoch 29/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6769 - accuracy: 0.5629\n",
      "Epoch 30/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6533 - accuracy: 0.5658\n",
      "Epoch 31/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6301 - accuracy: 0.5770\n",
      "Epoch 32/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6072 - accuracy: 0.5818\n",
      "Epoch 33/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5849 - accuracy: 0.5886\n",
      "Epoch 34/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5623 - accuracy: 0.5930\n",
      "Epoch 35/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5420 - accuracy: 0.5993\n",
      "Epoch 36/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5209 - accuracy: 0.6037\n",
      "Epoch 37/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.5001 - accuracy: 0.6124\n",
      "Epoch 38/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.4794 - accuracy: 0.6178\n",
      "Epoch 39/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4600 - accuracy: 0.6226\n",
      "Epoch 40/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4403 - accuracy: 0.6260\n",
      "Epoch 41/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.4228 - accuracy: 0.6328\n",
      "Epoch 42/50\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 1.4033 - accuracy: 0.6396\n",
      "Epoch 43/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3856 - accuracy: 0.6440\n",
      "Epoch 44/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3676 - accuracy: 0.6479\n",
      "Epoch 45/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3499 - accuracy: 0.6493\n",
      "Epoch 46/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3332 - accuracy: 0.6571\n",
      "Epoch 47/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3157 - accuracy: 0.6595\n",
      "Epoch 48/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2984 - accuracy: 0.6644\n",
      "Epoch 49/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2833 - accuracy: 0.6659\n",
      "Epoch 50/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2674 - accuracy: 0.6702\n",
      "1th iteration done\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.19143125561693078\n",
      "Validation Accuracy: 0.22641509433962265\n",
      "Shapes before scaling:\n",
      "X_train: (2025, 384)\n",
      "X_test: (299, 384)\n",
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 7ms/step - loss: 2.7947 - accuracy: 0.0602\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.6230 - accuracy: 0.1644\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.5506 - accuracy: 0.2267\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.4928 - accuracy: 0.2385\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.4407 - accuracy: 0.2672\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 2.3939 - accuracy: 0.2835\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.3515 - accuracy: 0.3244\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.3107 - accuracy: 0.3373\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.2721 - accuracy: 0.3654\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.2363 - accuracy: 0.3635\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.2009 - accuracy: 0.3906\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1682 - accuracy: 0.3965\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1339 - accuracy: 0.3941\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1008 - accuracy: 0.4168\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.0683 - accuracy: 0.4217\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.0381 - accuracy: 0.4346\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.0067 - accuracy: 0.4346\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9767 - accuracy: 0.4459\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.9465 - accuracy: 0.4637\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9180 - accuracy: 0.4751\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8899 - accuracy: 0.4830\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8614 - accuracy: 0.4859\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8335 - accuracy: 0.5017\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.8070 - accuracy: 0.5101\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7821 - accuracy: 0.5136\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7543 - accuracy: 0.5333\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7291 - accuracy: 0.5378\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7033 - accuracy: 0.5481\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6799 - accuracy: 0.5610\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.6562 - accuracy: 0.5644\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.6327 - accuracy: 0.5728\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6105 - accuracy: 0.5738\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5873 - accuracy: 0.5872\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.5676 - accuracy: 0.5921\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5460 - accuracy: 0.5975\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.5246 - accuracy: 0.6015\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5048 - accuracy: 0.6059\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4840 - accuracy: 0.6153\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4646 - accuracy: 0.6168\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.4454 - accuracy: 0.6237\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.4265 - accuracy: 0.6217\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.4082 - accuracy: 0.6326\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3898 - accuracy: 0.6469\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3727 - accuracy: 0.6489\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3556 - accuracy: 0.6474\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3384 - accuracy: 0.6588\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3214 - accuracy: 0.6598\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.3056 - accuracy: 0.6642\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2904 - accuracy: 0.6721\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 1.2736 - accuracy: 0.6770\n",
      "2th iteration done\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.24634103532099455\n",
      "Validation Accuracy: 0.2608695652173913\n",
      "Shapes before scaling:\n",
      "X_train: (1956, 384)\n",
      "X_test: (368, 384)\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 1s 7ms/step - loss: 2.7195 - accuracy: 0.0869\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.6075 - accuracy: 0.1738\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.5492 - accuracy: 0.2347\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.4965 - accuracy: 0.2556\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.4461 - accuracy: 0.3027\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.4000 - accuracy: 0.3124\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.3591 - accuracy: 0.3226\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.3201 - accuracy: 0.3257\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.2836 - accuracy: 0.3553\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.2502 - accuracy: 0.3676\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.2165 - accuracy: 0.3753\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.1860 - accuracy: 0.3870\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.1545 - accuracy: 0.3926\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.1238 - accuracy: 0.4131\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.0944 - accuracy: 0.4197\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.0666 - accuracy: 0.4376\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.0386 - accuracy: 0.4550\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.0105 - accuracy: 0.4596\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.9820 - accuracy: 0.4627\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.9558 - accuracy: 0.4770\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.9308 - accuracy: 0.4806\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.9052 - accuracy: 0.5026\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.8787 - accuracy: 0.5138\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.8540 - accuracy: 0.5107\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.8289 - accuracy: 0.5327\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.8059 - accuracy: 0.5204\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.7818 - accuracy: 0.5373\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.7579 - accuracy: 0.5409\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.7368 - accuracy: 0.5527\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.7134 - accuracy: 0.5649\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.6910 - accuracy: 0.5685\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.6695 - accuracy: 0.5706\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.6490 - accuracy: 0.5736\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.6282 - accuracy: 0.5782\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 0s 4ms/step - loss: 1.6077 - accuracy: 0.5879\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5866 - accuracy: 0.5925\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5678 - accuracy: 0.6048\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5484 - accuracy: 0.6058\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5295 - accuracy: 0.6074\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5123 - accuracy: 0.6155\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4941 - accuracy: 0.6232\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4756 - accuracy: 0.6237\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4573 - accuracy: 0.6324\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4406 - accuracy: 0.6370\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4245 - accuracy: 0.6421\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4075 - accuracy: 0.6452\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.3915 - accuracy: 0.6524\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.3751 - accuracy: 0.6575\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.3589 - accuracy: 0.6605\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.3442 - accuracy: 0.6621\n",
      "3th iteration done\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.1723842757639059\n",
      "Validation Accuracy: 0.1956521739130435\n",
      "Shapes before scaling:\n",
      "X_train: (1927, 384)\n",
      "X_test: (397, 384)\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 1s 7ms/step - loss: 2.8252 - accuracy: 0.0794\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.6197 - accuracy: 0.1495\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.5420 - accuracy: 0.2211\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.4844 - accuracy: 0.2465\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.4339 - accuracy: 0.2922\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.3881 - accuracy: 0.3005\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.3449 - accuracy: 0.3062\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.3067 - accuracy: 0.3508\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.2692 - accuracy: 0.3581\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.2334 - accuracy: 0.3793\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.1997 - accuracy: 0.3830\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.1662 - accuracy: 0.3845\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.1347 - accuracy: 0.3954\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.1028 - accuracy: 0.4167\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.0721 - accuracy: 0.4468\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.0406 - accuracy: 0.4390\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.0113 - accuracy: 0.4453\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.9811 - accuracy: 0.4593\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.9524 - accuracy: 0.4759\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.9229 - accuracy: 0.4899\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.8958 - accuracy: 0.4883\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.8679 - accuracy: 0.5112\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.8413 - accuracy: 0.5132\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.8142 - accuracy: 0.5215\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.7894 - accuracy: 0.5272\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.7640 - accuracy: 0.5345\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.7385 - accuracy: 0.5496\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.7145 - accuracy: 0.5480\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.6897 - accuracy: 0.5688\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.6657 - accuracy: 0.5636\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.6448 - accuracy: 0.5724\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.6217 - accuracy: 0.5869\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.5992 - accuracy: 0.5874\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.5783 - accuracy: 0.5921\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.5562 - accuracy: 0.6051\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.5368 - accuracy: 0.6072\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.5160 - accuracy: 0.6155\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4969 - accuracy: 0.6170\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4774 - accuracy: 0.6232\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4576 - accuracy: 0.6326\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4392 - accuracy: 0.6326\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4208 - accuracy: 0.6362\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4026 - accuracy: 0.6404\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3856 - accuracy: 0.6502\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3682 - accuracy: 0.6528\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3514 - accuracy: 0.6585\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3343 - accuracy: 0.6617\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3186 - accuracy: 0.6710\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3030 - accuracy: 0.6710\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 1.2865 - accuracy: 0.6772\n",
      "4th iteration done\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.1388773806310299\n",
      "Validation Accuracy: 0.15869017632241814\n",
      "Shapes before scaling:\n",
      "X_train: (2051, 384)\n",
      "X_test: (273, 384)\n",
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 7ms/step - loss: 2.7240 - accuracy: 0.0775\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.6142 - accuracy: 0.1385\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.5546 - accuracy: 0.1872\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.4975 - accuracy: 0.2496\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.4460 - accuracy: 0.2750\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.3966 - accuracy: 0.3057\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.3536 - accuracy: 0.3145\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.3135 - accuracy: 0.3228\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.2757 - accuracy: 0.3374\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.2393 - accuracy: 0.3564\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.2047 - accuracy: 0.3735\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.1706 - accuracy: 0.3881\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.1387 - accuracy: 0.3979\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.1068 - accuracy: 0.4144\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0757 - accuracy: 0.4149\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0450 - accuracy: 0.4334\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0139 - accuracy: 0.4481\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9841 - accuracy: 0.4569\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9558 - accuracy: 0.4627\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9275 - accuracy: 0.4861\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8994 - accuracy: 0.4827\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8722 - accuracy: 0.4993\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8452 - accuracy: 0.5100\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8184 - accuracy: 0.5154\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7927 - accuracy: 0.5232\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7662 - accuracy: 0.5295\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7415 - accuracy: 0.5358\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7166 - accuracy: 0.5466\n",
      "Epoch 29/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6935 - accuracy: 0.5588\n",
      "Epoch 30/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6686 - accuracy: 0.5588\n",
      "Epoch 31/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6452 - accuracy: 0.5680\n",
      "Epoch 32/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6227 - accuracy: 0.5822\n",
      "Epoch 33/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6010 - accuracy: 0.5831\n",
      "Epoch 34/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5794 - accuracy: 0.5943\n",
      "Epoch 35/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5582 - accuracy: 0.6017\n",
      "Epoch 36/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5368 - accuracy: 0.6090\n",
      "Epoch 37/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5167 - accuracy: 0.6124\n",
      "Epoch 38/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4973 - accuracy: 0.6168\n",
      "Epoch 39/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4766 - accuracy: 0.6216\n",
      "Epoch 40/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4577 - accuracy: 0.6246\n",
      "Epoch 41/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4400 - accuracy: 0.6275\n",
      "Epoch 42/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4211 - accuracy: 0.6324\n",
      "Epoch 43/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4029 - accuracy: 0.6387\n",
      "Epoch 44/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3856 - accuracy: 0.6421\n",
      "Epoch 45/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3679 - accuracy: 0.6475\n",
      "Epoch 46/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3517 - accuracy: 0.6533\n",
      "Epoch 47/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3347 - accuracy: 0.6582\n",
      "Epoch 48/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3186 - accuracy: 0.6611\n",
      "Epoch 49/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3024 - accuracy: 0.6636\n",
      "Epoch 50/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2869 - accuracy: 0.6714\n",
      "5th iteration done\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "Validation F1 Score (macro): 0.1609596188508677\n",
      "Validation Accuracy: 0.17216117216117216\n",
      "Shapes before scaling:\n",
      "X_train: (1904, 384)\n",
      "X_test: (420, 384)\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 1s 7ms/step - loss: 2.7644 - accuracy: 0.0987\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.6190 - accuracy: 0.1523\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.5472 - accuracy: 0.1728\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.4923 - accuracy: 0.1954\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.4437 - accuracy: 0.2285\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.4031 - accuracy: 0.2353\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.3649 - accuracy: 0.2574\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.3318 - accuracy: 0.3030\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.3003 - accuracy: 0.3093\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.2702 - accuracy: 0.3309\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.2405 - accuracy: 0.3445\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.2123 - accuracy: 0.3550\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1839 - accuracy: 0.3797\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1563 - accuracy: 0.3923\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1286 - accuracy: 0.4013\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1018 - accuracy: 0.4270\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0741 - accuracy: 0.4322\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0467 - accuracy: 0.4443\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0212 - accuracy: 0.4548\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9956 - accuracy: 0.4606\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9695 - accuracy: 0.4737\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9432 - accuracy: 0.4811\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9190 - accuracy: 0.4942\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8931 - accuracy: 0.4947\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8688 - accuracy: 0.4989\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8444 - accuracy: 0.5121\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8214 - accuracy: 0.5200\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7982 - accuracy: 0.5231\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7744 - accuracy: 0.5389\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7522 - accuracy: 0.5425\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7312 - accuracy: 0.5467\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7088 - accuracy: 0.5494\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6873 - accuracy: 0.5567\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6673 - accuracy: 0.5614\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6456 - accuracy: 0.5683\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6245 - accuracy: 0.5693\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6060 - accuracy: 0.5704\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5861 - accuracy: 0.5830\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5671 - accuracy: 0.5783\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5481 - accuracy: 0.5924\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5294 - accuracy: 0.5961\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5109 - accuracy: 0.6024\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4933 - accuracy: 0.6035\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4756 - accuracy: 0.6150\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4584 - accuracy: 0.6213\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 1.4419 - accuracy: 0.6192\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4240 - accuracy: 0.6250\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4081 - accuracy: 0.6329\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3918 - accuracy: 0.6434\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3765 - accuracy: 0.6392\n",
      "6th iteration done\n",
      "14/14 [==============================] - 0s 2ms/step\n",
      "Validation F1 Score (macro): 0.21107529913725007\n",
      "Validation Accuracy: 0.25476190476190474\n"
     ]
    }
   ],
   "source": [
    "#try neural nets\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def build_mlp_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(256, activation='sigmoid', input_shape=(input_shape,)),\n",
    "        layers.Dense(128, activation='sigmoid'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#Remove one speaker for cross validation\n",
    "cols=smile_norm.columns\n",
    "scores_nn=pd.DataFrame(columns=['speaker', 'accuracy', 'F1', 'n'])\n",
    "i=0\n",
    "for speaker in smile_norm['speaker'].unique():\n",
    "    # Filter the data for the current speaker\n",
    "    test = smile_norm[smile_norm['speaker'] == speaker]\n",
    "    # Filter the remaining data for training\n",
    "    train = smile_norm[smile_norm['speaker'] != speaker]\n",
    "    features= list(train.columns[4:])\n",
    "\n",
    "    X_train= train[features]\n",
    "    y_train= train['emotion']\n",
    "\n",
    "    X_test= test[features]\n",
    "    y_test= test['emotion']\n",
    "    \n",
    "    #only OpenSmile\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "    \n",
    "    print(\"Shapes before scaling:\")\n",
    "    print(\"X_train:\", X_train.shape)\n",
    "    print(\"X_test:\", X_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = build_mlp_model(input_shape=X_train.shape[1], num_classes=num_classes)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "    print(f\"{i}th iteration done\")\n",
    "    y_val_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_val_pred)\n",
    "    f1 = f1_score(y_test, y_val_pred, average='weighted')\n",
    "\n",
    "    print(\"Validation F1 Score (macro):\", f1)\n",
    "    print(\"Validation Accuracy:\", accuracy)\n",
    "    n=test.shape[0]\n",
    "    \n",
    "    row=[speaker, accuracy, f1, n]\n",
    "    scores_nn.loc[i]=row\n",
    "    i+=1\n",
    "    if f1 < 0.20 and i <= 1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "43fa6274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mm</td>\n",
       "      <td>0.314570</td>\n",
       "      <td>0.300965</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.191431</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mf</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.246341</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cl</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.172384</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mk</td>\n",
       "      <td>0.158690</td>\n",
       "      <td>0.138877</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jg</td>\n",
       "      <td>0.172161</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gg</td>\n",
       "      <td>0.254762</td>\n",
       "      <td>0.211075</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker  accuracy        F1    n\n",
       "0      mm  0.314570  0.300965  302\n",
       "1      cc  0.226415  0.191431  265\n",
       "2      mf  0.260870  0.246341  299\n",
       "3      cl  0.195652  0.172384  368\n",
       "4      mk  0.158690  0.138877  397\n",
       "5      jg  0.172161  0.160960  273\n",
       "6      gg  0.254762  0.211075  420"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bdeeeccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Average Accuracry:  0.22461273666092943\n",
      "Aggreaged F1:  0.20070659093503054\n"
     ]
    }
   ],
   "source": [
    "assert(scores_nn['n'].sum()==2324)\n",
    "n_total= scores_nn['n'].sum()\n",
    "accuracy=0\n",
    "F1=0\n",
    "#will run 7 times\n",
    "for index, row in scores_nn.iterrows():\n",
    "    speaker= row['speaker']\n",
    "    acc= row['accuracy']\n",
    "    f1= row['F1']\n",
    "    n= row['n']\n",
    "    accuracy+= acc * n\n",
    "    F1+= f1 * n\n",
    "\n",
    "aggregated_accuracy= accuracy/n_total\n",
    "aggregated_F1= F1/n_total\n",
    "\n",
    "print(\"Aggregated Average Accuracry: \", aggregated_accuracy)\n",
    "print(\"Aggreaged F1: \", aggregated_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ea14178e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before scaling:\n",
      "X_train: (2022, 384)\n",
      "X_test: (302, 384)\n",
      "Epoch 1/50\n",
      "64/64 [==============================] - 2s 18ms/step - loss: 2.7696 - accuracy: 0.0806\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.6406 - accuracy: 0.1137\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.5465 - accuracy: 0.1528\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.4906 - accuracy: 0.1756\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.4238 - accuracy: 0.1988\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.3584 - accuracy: 0.2265\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.3252 - accuracy: 0.2349\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.2784 - accuracy: 0.2641\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.2245 - accuracy: 0.2982\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1907 - accuracy: 0.3121\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.1378 - accuracy: 0.3398\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.1060 - accuracy: 0.3417\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.0645 - accuracy: 0.3640\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.0383 - accuracy: 0.3704\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9917 - accuracy: 0.4139\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9674 - accuracy: 0.4139\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.9190 - accuracy: 0.4308\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.8912 - accuracy: 0.4382\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.8620 - accuracy: 0.4496\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8238 - accuracy: 0.4748\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7847 - accuracy: 0.4985\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7627 - accuracy: 0.4955\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.7230 - accuracy: 0.5168\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7011 - accuracy: 0.5114\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6763 - accuracy: 0.5203\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6576 - accuracy: 0.5232\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6254 - accuracy: 0.5371\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5959 - accuracy: 0.5455\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5661 - accuracy: 0.5524\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.5413 - accuracy: 0.5643\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5172 - accuracy: 0.5752\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4984 - accuracy: 0.5796\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4745 - accuracy: 0.5851\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4580 - accuracy: 0.5895\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4327 - accuracy: 0.6083\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4175 - accuracy: 0.6014\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3914 - accuracy: 0.6063\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3723 - accuracy: 0.6137\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3555 - accuracy: 0.6128\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3448 - accuracy: 0.6162\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3214 - accuracy: 0.6355\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2998 - accuracy: 0.6340\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2760 - accuracy: 0.6469\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2620 - accuracy: 0.6454\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2509 - accuracy: 0.6429\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2389 - accuracy: 0.6494\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2149 - accuracy: 0.6573\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2061 - accuracy: 0.6528\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1925 - accuracy: 0.6578\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1764 - accuracy: 0.6622\n",
      "0th iteration done\n",
      "10/10 [==============================] - 0s 22ms/step\n",
      "Validation F1 Score (macro): 0.3035148376374452\n",
      "Validation Accuracy: 0.31788079470198677\n",
      "Shapes before scaling:\n",
      "X_train: (2059, 384)\n",
      "X_test: (265, 384)\n",
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 2.7633 - accuracy: 0.0801\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.6489 - accuracy: 0.1030\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.5484 - accuracy: 0.1462\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.4668 - accuracy: 0.1778\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.3981 - accuracy: 0.2054\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.3451 - accuracy: 0.2351\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.2933 - accuracy: 0.2637\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 2.2522 - accuracy: 0.2861\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.2027 - accuracy: 0.3162\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.1715 - accuracy: 0.3210\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.1008 - accuracy: 0.3720\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.0638 - accuracy: 0.3856\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0338 - accuracy: 0.3987\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9927 - accuracy: 0.3963\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9458 - accuracy: 0.4298\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9061 - accuracy: 0.4468\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8793 - accuracy: 0.4599\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8442 - accuracy: 0.4492\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.8058 - accuracy: 0.4842\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7714 - accuracy: 0.4847\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7413 - accuracy: 0.5036\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7056 - accuracy: 0.5182\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6683 - accuracy: 0.5153\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6470 - accuracy: 0.5391\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.6296 - accuracy: 0.5333\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5913 - accuracy: 0.5571\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5600 - accuracy: 0.5658\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5397 - accuracy: 0.5658\n",
      "Epoch 29/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5103 - accuracy: 0.5697\n",
      "Epoch 30/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4844 - accuracy: 0.5911\n",
      "Epoch 31/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4571 - accuracy: 0.5852\n",
      "Epoch 32/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4308 - accuracy: 0.6022\n",
      "Epoch 33/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4132 - accuracy: 0.6134\n",
      "Epoch 34/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3980 - accuracy: 0.6105\n",
      "Epoch 35/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3722 - accuracy: 0.6192\n",
      "Epoch 36/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3462 - accuracy: 0.6323\n",
      "Epoch 37/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3291 - accuracy: 0.6425\n",
      "Epoch 38/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3021 - accuracy: 0.6557\n",
      "Epoch 39/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2886 - accuracy: 0.6406\n",
      "Epoch 40/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2643 - accuracy: 0.6571\n",
      "Epoch 41/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2506 - accuracy: 0.6498\n",
      "Epoch 42/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2329 - accuracy: 0.6625\n",
      "Epoch 43/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2158 - accuracy: 0.6634\n",
      "Epoch 44/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1943 - accuracy: 0.6741\n",
      "Epoch 45/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1775 - accuracy: 0.6819\n",
      "Epoch 46/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1623 - accuracy: 0.6780\n",
      "Epoch 47/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1509 - accuracy: 0.6848\n",
      "Epoch 48/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1324 - accuracy: 0.6843\n",
      "Epoch 49/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1152 - accuracy: 0.6974\n",
      "Epoch 50/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0983 - accuracy: 0.7042\n",
      "1th iteration done\n",
      "9/9 [==============================] - 0s 23ms/step\n",
      "Validation F1 Score (macro): 0.19764716836984547\n",
      "Validation Accuracy: 0.23018867924528302\n",
      "Shapes before scaling:\n",
      "X_train: (2025, 384)\n",
      "X_test: (299, 384)\n",
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 11ms/step - loss: 2.8218 - accuracy: 0.0785\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.6487 - accuracy: 0.1230\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.5457 - accuracy: 0.1699\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.4830 - accuracy: 0.1872\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.4110 - accuracy: 0.2109\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 2.3573 - accuracy: 0.2464\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.3039 - accuracy: 0.2632\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.2435 - accuracy: 0.2904\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.2114 - accuracy: 0.3180\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.1496 - accuracy: 0.3486\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.1099 - accuracy: 0.3585\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.0858 - accuracy: 0.3556\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.0380 - accuracy: 0.3842\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.9964 - accuracy: 0.4005\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.9661 - accuracy: 0.4074\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9240 - accuracy: 0.4415\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8853 - accuracy: 0.4365\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8507 - accuracy: 0.4514\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.8204 - accuracy: 0.4543\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7898 - accuracy: 0.4805\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7458 - accuracy: 0.4923\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.7158 - accuracy: 0.5111\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6841 - accuracy: 0.5220\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6573 - accuracy: 0.5200\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.6292 - accuracy: 0.5299\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5977 - accuracy: 0.5383\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5677 - accuracy: 0.5501\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5457 - accuracy: 0.5600\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.5202 - accuracy: 0.5881\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4907 - accuracy: 0.5778\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4722 - accuracy: 0.5867\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4542 - accuracy: 0.5956\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4236 - accuracy: 0.6020\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.4070 - accuracy: 0.6035\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3818 - accuracy: 0.6183\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3507 - accuracy: 0.6267\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.3490 - accuracy: 0.6217\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3119 - accuracy: 0.6400\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3014 - accuracy: 0.6410\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2789 - accuracy: 0.6400\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2618 - accuracy: 0.6528\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2423 - accuracy: 0.6489\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2269 - accuracy: 0.6553\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2091 - accuracy: 0.6672\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1861 - accuracy: 0.6721\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1746 - accuracy: 0.6790\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1618 - accuracy: 0.6780\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1497 - accuracy: 0.6815\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1368 - accuracy: 0.6938\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.1177 - accuracy: 0.6899\n",
      "2th iteration done\n",
      "10/10 [==============================] - 0s 4ms/step\n",
      "Validation F1 Score (macro): 0.2108787478404283\n",
      "Validation Accuracy: 0.22073578595317725\n",
      "Shapes before scaling:\n",
      "X_train: (1956, 384)\n",
      "X_test: (368, 384)\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.8009 - accuracy: 0.0828\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.6336 - accuracy: 0.1247\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.5693 - accuracy: 0.1518\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.4876 - accuracy: 0.1825\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.4329 - accuracy: 0.2019\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.3760 - accuracy: 0.2367\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.3065 - accuracy: 0.2791\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.2739 - accuracy: 0.2960\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.2277 - accuracy: 0.3001\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.1868 - accuracy: 0.3216\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.1384 - accuracy: 0.3548\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.1095 - accuracy: 0.3635\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 2.0660 - accuracy: 0.3788\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.0324 - accuracy: 0.3967\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.9916 - accuracy: 0.4075\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.9579 - accuracy: 0.4330\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.9247 - accuracy: 0.4565\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.8881 - accuracy: 0.4668\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.8552 - accuracy: 0.4734\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.8263 - accuracy: 0.4693\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.7916 - accuracy: 0.4954\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.7519 - accuracy: 0.5138\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.7283 - accuracy: 0.5066\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.6975 - accuracy: 0.5184\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.6641 - accuracy: 0.5322\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.6389 - accuracy: 0.5368\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.6155 - accuracy: 0.5481\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5973 - accuracy: 0.5527\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5694 - accuracy: 0.5665\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5445 - accuracy: 0.5665\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.5148 - accuracy: 0.5782\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4991 - accuracy: 0.5792\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.4686 - accuracy: 0.5966\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4482 - accuracy: 0.5895\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4264 - accuracy: 0.5966\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.4079 - accuracy: 0.6038\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.3882 - accuracy: 0.6063\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.3704 - accuracy: 0.6145\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.3442 - accuracy: 0.6309\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.3277 - accuracy: 0.6258\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.3056 - accuracy: 0.6447\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.2980 - accuracy: 0.6334\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.2843 - accuracy: 0.6411\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.2637 - accuracy: 0.6508\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.2490 - accuracy: 0.6575\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.2310 - accuracy: 0.6585\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.2139 - accuracy: 0.6636\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 1.2068 - accuracy: 0.6667\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.1827 - accuracy: 0.6662\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.1685 - accuracy: 0.6820\n",
      "3th iteration done\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "Validation F1 Score (macro): 0.15821670763137816\n",
      "Validation Accuracy: 0.18206521739130435\n",
      "Shapes before scaling:\n",
      "X_train: (1927, 384)\n",
      "X_test: (397, 384)\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 1s 14ms/step - loss: 2.8243 - accuracy: 0.0768\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.6331 - accuracy: 0.1225\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.5632 - accuracy: 0.1526\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.4769 - accuracy: 0.1728\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.4205 - accuracy: 0.1993\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.3573 - accuracy: 0.2506\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.3176 - accuracy: 0.2569\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.2798 - accuracy: 0.2626\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.2249 - accuracy: 0.2958\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.1738 - accuracy: 0.3259\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.1355 - accuracy: 0.3280\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.0909 - accuracy: 0.3757\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 2.0613 - accuracy: 0.3845\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.0163 - accuracy: 0.3876\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.9783 - accuracy: 0.4261\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.9430 - accuracy: 0.4240\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.9032 - accuracy: 0.4390\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.8608 - accuracy: 0.4754\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.8330 - accuracy: 0.4795\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.7973 - accuracy: 0.4899\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.7596 - accuracy: 0.5112\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.7344 - accuracy: 0.5132\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.7050 - accuracy: 0.5215\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.6784 - accuracy: 0.5340\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.6440 - accuracy: 0.5439\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.6066 - accuracy: 0.5620\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.5717 - accuracy: 0.5615\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.5506 - accuracy: 0.5781\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.5823\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.4983 - accuracy: 0.5890\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4696 - accuracy: 0.6035\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4518 - accuracy: 0.5973\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4187 - accuracy: 0.6118\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.4053 - accuracy: 0.6056\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3872 - accuracy: 0.6196\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3541 - accuracy: 0.6357\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.3383 - accuracy: 0.6232\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3178 - accuracy: 0.6450\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.3009 - accuracy: 0.6450\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.2734 - accuracy: 0.6653\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.2520 - accuracy: 0.6637\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.2397 - accuracy: 0.6648\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.2230 - accuracy: 0.6689\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.2079 - accuracy: 0.6757\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1906 - accuracy: 0.6881\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.1721 - accuracy: 0.6809\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1582 - accuracy: 0.6902\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1410 - accuracy: 0.6912\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1237 - accuracy: 0.7037\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.1141 - accuracy: 0.7016\n",
      "4th iteration done\n",
      "13/13 [==============================] - 0s 16ms/step\n",
      "Validation F1 Score (macro): 0.18452260742970208\n",
      "Validation Accuracy: 0.2040302267002519\n",
      "Shapes before scaling:\n",
      "X_train: (2051, 384)\n",
      "X_test: (273, 384)\n",
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 2.8157 - accuracy: 0.0751\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.6227 - accuracy: 0.1282\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.5328 - accuracy: 0.1541\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.4576 - accuracy: 0.1858\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.3952 - accuracy: 0.2014\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.3391 - accuracy: 0.2262\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.2894 - accuracy: 0.2535\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.2378 - accuracy: 0.2882\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.1976 - accuracy: 0.3023\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.1525 - accuracy: 0.3291\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.1142 - accuracy: 0.3535\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0844 - accuracy: 0.3647\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 2.0373 - accuracy: 0.3730\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9973 - accuracy: 0.3988\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.9609 - accuracy: 0.4203\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.9165 - accuracy: 0.4383\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8925 - accuracy: 0.4354\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.8581 - accuracy: 0.4510\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.8231 - accuracy: 0.4588\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7884 - accuracy: 0.4807\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7528 - accuracy: 0.4963\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.7255 - accuracy: 0.5027\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6958 - accuracy: 0.5149\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.6591 - accuracy: 0.5202\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.6308 - accuracy: 0.5368\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5972 - accuracy: 0.5495\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5888 - accuracy: 0.5358\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.5455 - accuracy: 0.5636\n",
      "Epoch 29/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.5181 - accuracy: 0.5841\n",
      "Epoch 30/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4998 - accuracy: 0.5919\n",
      "Epoch 31/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4705 - accuracy: 0.5904\n",
      "Epoch 32/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4486 - accuracy: 0.6085\n",
      "Epoch 33/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4231 - accuracy: 0.6007\n",
      "Epoch 34/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4066 - accuracy: 0.6007\n",
      "Epoch 35/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3868 - accuracy: 0.6202\n",
      "Epoch 36/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3556 - accuracy: 0.6324\n",
      "Epoch 37/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3394 - accuracy: 0.6241\n",
      "Epoch 38/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3167 - accuracy: 0.6407\n",
      "Epoch 39/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2996 - accuracy: 0.6490\n",
      "Epoch 40/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2873 - accuracy: 0.6402\n",
      "Epoch 41/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2639 - accuracy: 0.6572\n",
      "Epoch 42/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2436 - accuracy: 0.6650\n",
      "Epoch 43/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2272 - accuracy: 0.6616\n",
      "Epoch 44/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2042 - accuracy: 0.6738\n",
      "Epoch 45/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1951 - accuracy: 0.6753\n",
      "Epoch 46/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.1771 - accuracy: 0.6738\n",
      "Epoch 47/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1582 - accuracy: 0.6826\n",
      "Epoch 48/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1440 - accuracy: 0.6865\n",
      "Epoch 49/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1320 - accuracy: 0.6904\n",
      "Epoch 50/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.1178 - accuracy: 0.6899\n",
      "5th iteration done\n",
      "9/9 [==============================] - 0s 6ms/step\n",
      "Validation F1 Score (macro): 0.1556225623530817\n",
      "Validation Accuracy: 0.1684981684981685\n",
      "Shapes before scaling:\n",
      "X_train: (1904, 384)\n",
      "X_test: (420, 384)\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.7744 - accuracy: 0.0819\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.6368 - accuracy: 0.1124\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.5404 - accuracy: 0.1639\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.4841 - accuracy: 0.1733\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4256 - accuracy: 0.2022\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3613 - accuracy: 0.2227\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3144 - accuracy: 0.2348\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2863 - accuracy: 0.2442\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2395 - accuracy: 0.2768\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.2099 - accuracy: 0.2836\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1696 - accuracy: 0.3283\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.1328 - accuracy: 0.3298\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.0968 - accuracy: 0.3461\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0664 - accuracy: 0.3592\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 2.0247 - accuracy: 0.3818\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9934 - accuracy: 0.3992\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9576 - accuracy: 0.4181\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9297 - accuracy: 0.4149\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.9024 - accuracy: 0.4370\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8694 - accuracy: 0.4422\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8328 - accuracy: 0.4606\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.8019 - accuracy: 0.4690\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7799 - accuracy: 0.4811\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7470 - accuracy: 0.4779\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.7224 - accuracy: 0.4953\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6932 - accuracy: 0.5200\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6707 - accuracy: 0.5205\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.6394 - accuracy: 0.5336\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.6090 - accuracy: 0.5378\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5800 - accuracy: 0.5499\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5717 - accuracy: 0.5588\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5419 - accuracy: 0.5693\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.5204 - accuracy: 0.5672\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4963 - accuracy: 0.5777\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4711 - accuracy: 0.5814\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4507 - accuracy: 0.5972\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.4296 - accuracy: 0.5898\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.4226 - accuracy: 0.6035\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3920 - accuracy: 0.6092\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3772 - accuracy: 0.6045\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3563 - accuracy: 0.6103\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.3394 - accuracy: 0.6150\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.3161 - accuracy: 0.6203\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.3061 - accuracy: 0.6313\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2856 - accuracy: 0.6439\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.2672 - accuracy: 0.6444\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2431 - accuracy: 0.6434\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.2306 - accuracy: 0.6602\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.2152 - accuracy: 0.6623\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.2097 - accuracy: 0.6660\n",
      "6th iteration done\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.23980175919900076\n",
      "Validation Accuracy: 0.27380952380952384\n"
     ]
    }
   ],
   "source": [
    "#try neural nets\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def build_mlp_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(512, activation='sigmoid', input_shape=(input_shape,)),\n",
    "        Dropout(0.25),\n",
    "        layers.Dense(256, activation='sigmoid'),\n",
    "        Dropout(0.25),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#Remove one speaker for cross validation\n",
    "cols=smile_norm.columns\n",
    "scores_nn=pd.DataFrame(columns=['speaker', 'accuracy', 'F1', 'n'])\n",
    "i=0\n",
    "for speaker in smile_norm['speaker'].unique():\n",
    "    # Filter the data for the current speaker\n",
    "    test = smile_norm[smile_norm['speaker'] == speaker]\n",
    "    # Filter the remaining data for training\n",
    "    train = smile_norm[smile_norm['speaker'] != speaker]\n",
    "    features= list(train.columns[4:])\n",
    "\n",
    "    X_train= train[features]\n",
    "    y_train= train['emotion']\n",
    "\n",
    "    X_test= test[features]\n",
    "    y_test= test['emotion']\n",
    "    \n",
    "    #only OpenSmile\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "    \n",
    "    print(\"Shapes before scaling:\")\n",
    "    print(\"X_train:\", X_train.shape)\n",
    "    print(\"X_test:\", X_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = build_mlp_model(input_shape=X_train.shape[1], num_classes=num_classes)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "    print(f\"{i}th iteration done\")\n",
    "    y_val_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_val_pred)\n",
    "    f1 = f1_score(y_test, y_val_pred, average='weighted')\n",
    "\n",
    "    print(\"Validation F1 Score (macro):\", f1)\n",
    "    print(\"Validation Accuracy:\", accuracy)\n",
    "    n=test.shape[0]\n",
    "    \n",
    "    row=[speaker, accuracy, f1, n]\n",
    "    scores_nn.loc[i]=row\n",
    "    i+=1\n",
    "    if f1 < 0.25 and i <= 1: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "40fca64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mm</td>\n",
       "      <td>0.317881</td>\n",
       "      <td>0.303515</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc</td>\n",
       "      <td>0.230189</td>\n",
       "      <td>0.197647</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mf</td>\n",
       "      <td>0.220736</td>\n",
       "      <td>0.210879</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cl</td>\n",
       "      <td>0.182065</td>\n",
       "      <td>0.158217</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mk</td>\n",
       "      <td>0.204030</td>\n",
       "      <td>0.184523</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jg</td>\n",
       "      <td>0.168498</td>\n",
       "      <td>0.155623</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gg</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.239802</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker  accuracy        F1    n\n",
       "0      mm  0.317881  0.303515  302\n",
       "1      cc  0.230189  0.197647  265\n",
       "2      mf  0.220736  0.210879  299\n",
       "3      cl  0.182065  0.158217  368\n",
       "4      mk  0.204030  0.184523  397\n",
       "5      jg  0.168498  0.155623  273\n",
       "6      gg  0.273810  0.239802  420"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "11f789f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Average Accuracry:  0.2289156626506024\n",
      "Aggreaged F1:  0.20730277458378488\n"
     ]
    }
   ],
   "source": [
    "assert(scores_nn['n'].sum()==2324)\n",
    "n_total= scores_nn['n'].sum()\n",
    "accuracy=0\n",
    "F1=0\n",
    "#will run 7 times\n",
    "for index, row in scores_nn.iterrows():\n",
    "    speaker= row['speaker']\n",
    "    acc= row['accuracy']\n",
    "    f1= row['F1']\n",
    "    n= row['n']\n",
    "    accuracy+= acc * n\n",
    "    F1+= f1 * n\n",
    "\n",
    "aggregated_accuracy= accuracy/n_total\n",
    "aggregated_F1= F1/n_total\n",
    "\n",
    "print(\"Aggregated Average Accuracry: \", aggregated_accuracy)\n",
    "print(\"Aggreaged F1: \", aggregated_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "de943682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before scaling:\n",
      "X_train: (2022, 384)\n",
      "X_test: (302, 384)\n",
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 12ms/step - loss: 2.7599 - accuracy: 0.0905\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.5823 - accuracy: 0.1370\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.4794 - accuracy: 0.1845\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.3720 - accuracy: 0.2216\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.2802 - accuracy: 0.2651\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.2280 - accuracy: 0.2839\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.1598 - accuracy: 0.3136\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.1029 - accuracy: 0.3442\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.0409 - accuracy: 0.3635\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.9909 - accuracy: 0.3976\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.9371 - accuracy: 0.4041\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.8848 - accuracy: 0.4273\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.8278 - accuracy: 0.4629\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.7915 - accuracy: 0.4679\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.7322 - accuracy: 0.5025\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.6976 - accuracy: 0.5010\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.6592 - accuracy: 0.5163\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.6193 - accuracy: 0.5346\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.5772 - accuracy: 0.5430\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.5377 - accuracy: 0.5598\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.5074 - accuracy: 0.5712\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4739 - accuracy: 0.5801\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4447 - accuracy: 0.5920\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4124 - accuracy: 0.5915\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.3790 - accuracy: 0.6024\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.3472 - accuracy: 0.6162\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.3219 - accuracy: 0.6137\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.3088 - accuracy: 0.6202\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2787 - accuracy: 0.6311\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2566 - accuracy: 0.6375\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2353 - accuracy: 0.6306\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2159 - accuracy: 0.6548\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2002 - accuracy: 0.6484\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1738 - accuracy: 0.6642\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1522 - accuracy: 0.6622\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1392 - accuracy: 0.6622\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1231 - accuracy: 0.6701\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1004 - accuracy: 0.6835\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0769 - accuracy: 0.6860\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0698 - accuracy: 0.6820\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0565 - accuracy: 0.6899\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0404 - accuracy: 0.6978\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0209 - accuracy: 0.7023\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0100 - accuracy: 0.6983\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.9982 - accuracy: 0.7067\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9841 - accuracy: 0.7112\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9704 - accuracy: 0.7161\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9608 - accuracy: 0.7211\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9431 - accuracy: 0.7181\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9248 - accuracy: 0.7324\n",
      "0th iteration done\n",
      "10/10 [==============================] - 0s 4ms/step\n",
      "Validation F1 Score (macro): 0.28257929927314773\n",
      "Validation Accuracy: 0.30132450331125826\n",
      "Shapes before scaling:\n",
      "X_train: (2059, 384)\n",
      "X_test: (265, 384)\n",
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 2.7417 - accuracy: 0.0932\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.5611 - accuracy: 0.1374\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.4353 - accuracy: 0.1991\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.3288 - accuracy: 0.2482\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.2458 - accuracy: 0.2710\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.1813 - accuracy: 0.3079\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.1104 - accuracy: 0.3507\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.0475 - accuracy: 0.3895\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.9953 - accuracy: 0.4070\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.9368 - accuracy: 0.4216\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.8763 - accuracy: 0.4478\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.8155 - accuracy: 0.4672\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.7728 - accuracy: 0.4789\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.7204 - accuracy: 0.4925\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.6677 - accuracy: 0.5226\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.6201 - accuracy: 0.5381\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.5833 - accuracy: 0.5430\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.5431 - accuracy: 0.5605\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.5055 - accuracy: 0.5639\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.4650 - accuracy: 0.5881\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.4361 - accuracy: 0.5925\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3982 - accuracy: 0.6003\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.3642 - accuracy: 0.6105\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3435 - accuracy: 0.6129\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3069 - accuracy: 0.6241\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2806 - accuracy: 0.6353\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2511 - accuracy: 0.6450\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2239 - accuracy: 0.6586\n",
      "Epoch 29/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2120 - accuracy: 0.6435\n",
      "Epoch 30/50\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 1.1824 - accuracy: 0.6561\n",
      "Epoch 31/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.1577 - accuracy: 0.6668\n",
      "Epoch 32/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.1340 - accuracy: 0.6727\n",
      "Epoch 33/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.1129 - accuracy: 0.6795\n",
      "Epoch 34/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0957 - accuracy: 0.6804\n",
      "Epoch 35/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0799 - accuracy: 0.6897\n",
      "Epoch 36/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0571 - accuracy: 0.6965\n",
      "Epoch 37/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0455 - accuracy: 0.6969\n",
      "Epoch 38/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.0178 - accuracy: 0.7033\n",
      "Epoch 39/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0111 - accuracy: 0.7115\n",
      "Epoch 40/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9925 - accuracy: 0.7154\n",
      "Epoch 41/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9745 - accuracy: 0.7164\n",
      "Epoch 42/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9645 - accuracy: 0.7173\n",
      "Epoch 43/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.9484 - accuracy: 0.7300\n",
      "Epoch 44/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9310 - accuracy: 0.7266\n",
      "Epoch 45/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.9196 - accuracy: 0.7309\n",
      "Epoch 46/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9032 - accuracy: 0.7411\n",
      "Epoch 47/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.8903 - accuracy: 0.7416\n",
      "Epoch 48/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.8788 - accuracy: 0.7450\n",
      "Epoch 49/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.8630 - accuracy: 0.7499\n",
      "Epoch 50/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.8504 - accuracy: 0.7528\n",
      "1th iteration done\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "Validation F1 Score (macro): 0.23294414005160374\n",
      "Validation Accuracy: 0.25660377358490566\n",
      "Shapes before scaling:\n",
      "X_train: (2025, 384)\n",
      "X_test: (299, 384)\n",
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 11ms/step - loss: 2.7587 - accuracy: 0.0795\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.5759 - accuracy: 0.1541\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.4497 - accuracy: 0.2044\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.3439 - accuracy: 0.2390\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.2722 - accuracy: 0.2741\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.1860 - accuracy: 0.3126\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.1325 - accuracy: 0.3299\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 2.0552 - accuracy: 0.3832\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.9938 - accuracy: 0.3827\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.9377 - accuracy: 0.4089\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.8746 - accuracy: 0.4405\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.8204 - accuracy: 0.4652\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.7684 - accuracy: 0.4765\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.7210 - accuracy: 0.4983\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.6793 - accuracy: 0.4998\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.6336 - accuracy: 0.5225\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.5820 - accuracy: 0.5314\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.5479 - accuracy: 0.5511\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.5088 - accuracy: 0.5536\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4775 - accuracy: 0.5738\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4368 - accuracy: 0.5837\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.4062 - accuracy: 0.5970\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.3763 - accuracy: 0.5931\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.3499 - accuracy: 0.6114\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.3106 - accuracy: 0.6183\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2936 - accuracy: 0.6296\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2595 - accuracy: 0.6356\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.2346 - accuracy: 0.6400\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.2147 - accuracy: 0.6435\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1910 - accuracy: 0.6499\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1707 - accuracy: 0.6622\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1433 - accuracy: 0.6780\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1247 - accuracy: 0.6751\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.1020 - accuracy: 0.6830\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0864 - accuracy: 0.6835\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0641 - accuracy: 0.6998\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 1.0539 - accuracy: 0.6998\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 1.0372 - accuracy: 0.7032\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 1.0129 - accuracy: 0.7077\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9955 - accuracy: 0.7210\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9864 - accuracy: 0.7175\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9671 - accuracy: 0.7259\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9545 - accuracy: 0.7230\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9350 - accuracy: 0.7294\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9262 - accuracy: 0.7358\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9159 - accuracy: 0.7378\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.9015 - accuracy: 0.7333\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.8884 - accuracy: 0.7521\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.8871 - accuracy: 0.7447\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.8608 - accuracy: 0.7610\n",
      "2th iteration done\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.21844756988518083\n",
      "Validation Accuracy: 0.22408026755852842\n",
      "Shapes before scaling:\n",
      "X_train: (1956, 384)\n",
      "X_test: (368, 384)\n",
      "Epoch 1/50\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.7399 - accuracy: 0.0915\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.5557 - accuracy: 0.1636\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.4597 - accuracy: 0.1851\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.3607 - accuracy: 0.2372\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.2853 - accuracy: 0.2648\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.1992 - accuracy: 0.3154\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.1421 - accuracy: 0.3405\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.0855 - accuracy: 0.3686\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 2.0333 - accuracy: 0.3814\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.9741 - accuracy: 0.4141\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.9206 - accuracy: 0.4218\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.8719 - accuracy: 0.4484\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.8223 - accuracy: 0.4617\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.7688 - accuracy: 0.4852\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.7234 - accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.6965 - accuracy: 0.5026\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.6481 - accuracy: 0.5281\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.6118 - accuracy: 0.5302\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.5714 - accuracy: 0.5475\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.5383 - accuracy: 0.5567\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.4982 - accuracy: 0.5685\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.4741 - accuracy: 0.5787\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.4466 - accuracy: 0.5777\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.4195 - accuracy: 0.5956\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.3868 - accuracy: 0.6002\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.3552 - accuracy: 0.6115\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.3289 - accuracy: 0.6125\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.3052 - accuracy: 0.6319\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.2832 - accuracy: 0.6232\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.2632 - accuracy: 0.6355\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.2397 - accuracy: 0.6385\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.2194 - accuracy: 0.6503\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.1919 - accuracy: 0.6610\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.1774 - accuracy: 0.6651\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.1564 - accuracy: 0.6585\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.1422 - accuracy: 0.6748\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.1250 - accuracy: 0.6825\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.1075 - accuracy: 0.6840\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.0868 - accuracy: 0.6902\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.0712 - accuracy: 0.6994\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.0503 - accuracy: 0.6938\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.0398 - accuracy: 0.6953\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.0218 - accuracy: 0.7096\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 1.0084 - accuracy: 0.7086\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.9933 - accuracy: 0.7163\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.9823 - accuracy: 0.7147\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.9697 - accuracy: 0.7219\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.9542 - accuracy: 0.7239\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.9360 - accuracy: 0.7321\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 0.9342 - accuracy: 0.7372\n",
      "3th iteration done\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.1867073596096116\n",
      "Validation Accuracy: 0.20108695652173914\n",
      "Shapes before scaling:\n",
      "X_train: (1927, 384)\n",
      "X_test: (397, 384)\n",
      "Epoch 1/50\n",
      "61/61 [==============================] - 1s 12ms/step - loss: 2.7449 - accuracy: 0.0877\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.6127 - accuracy: 0.1173\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.4424 - accuracy: 0.2034\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 2.3536 - accuracy: 0.2273\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.2745 - accuracy: 0.2678\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 2.2008 - accuracy: 0.3062\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.1278 - accuracy: 0.3306\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 2.0759 - accuracy: 0.3674\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 2.0116 - accuracy: 0.3897\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.9424 - accuracy: 0.4193\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.8967 - accuracy: 0.4172\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.8282 - accuracy: 0.4759\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.7840 - accuracy: 0.4722\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.7297 - accuracy: 0.5029\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.6812 - accuracy: 0.5127\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.6403 - accuracy: 0.5283\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.5974 - accuracy: 0.5376\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.5445 - accuracy: 0.5605\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.5203 - accuracy: 0.5625\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 1.4735 - accuracy: 0.5823\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.4387 - accuracy: 0.5937\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.4029 - accuracy: 0.5989\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.3733 - accuracy: 0.6129\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.3436 - accuracy: 0.6160\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.3104 - accuracy: 0.6331\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.2882 - accuracy: 0.6274\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.2583 - accuracy: 0.6419\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.2383 - accuracy: 0.6497\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.2072 - accuracy: 0.6596\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.1885 - accuracy: 0.6544\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.1593 - accuracy: 0.6684\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.1391 - accuracy: 0.6746\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.1233 - accuracy: 0.6788\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.0956 - accuracy: 0.6855\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.0798 - accuracy: 0.6938\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.0630 - accuracy: 0.6949\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.0406 - accuracy: 0.7104\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 1.0348 - accuracy: 0.7037\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 1.0104 - accuracy: 0.7125\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.9905 - accuracy: 0.7213\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.9785 - accuracy: 0.7244\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.9657 - accuracy: 0.7260\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.9469 - accuracy: 0.7327\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.9364 - accuracy: 0.7343\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.9161 - accuracy: 0.7410\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.9090 - accuracy: 0.7400\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.8896 - accuracy: 0.7462\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.8787 - accuracy: 0.7504\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.8686 - accuracy: 0.7623\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.8495 - accuracy: 0.7654\n",
      "4th iteration done\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.17120355479500074\n",
      "Validation Accuracy: 0.19395465994962216\n",
      "Shapes before scaling:\n",
      "X_train: (2051, 384)\n",
      "X_test: (273, 384)\n",
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 2.7659 - accuracy: 0.0897\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.5644 - accuracy: 0.1409\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.4522 - accuracy: 0.1794\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.3446 - accuracy: 0.2389\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.2665 - accuracy: 0.2823\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.2084 - accuracy: 0.2921\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.1357 - accuracy: 0.3311\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.0733 - accuracy: 0.3754\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 2.0178 - accuracy: 0.3749\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.9589 - accuracy: 0.4139\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.9026 - accuracy: 0.4325\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.8450 - accuracy: 0.4393\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.7975 - accuracy: 0.4803\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.7518 - accuracy: 0.4895\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.7005 - accuracy: 0.5076\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.6575 - accuracy: 0.5232\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.6173 - accuracy: 0.5305\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.5759 - accuracy: 0.5588\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.5265 - accuracy: 0.5651\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.5035 - accuracy: 0.5641\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.4648 - accuracy: 0.5856\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.4359 - accuracy: 0.5919\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3943 - accuracy: 0.6012\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3679 - accuracy: 0.6163\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3381 - accuracy: 0.6216\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.3074 - accuracy: 0.6333\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.2782 - accuracy: 0.6368\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2478 - accuracy: 0.6509\n",
      "Epoch 29/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2230 - accuracy: 0.6524\n",
      "Epoch 30/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.2034 - accuracy: 0.6568\n",
      "Epoch 31/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.1786 - accuracy: 0.6655\n",
      "Epoch 32/50\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 1.1615 - accuracy: 0.6797\n",
      "Epoch 33/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.1435 - accuracy: 0.6753\n",
      "Epoch 34/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.1133 - accuracy: 0.6767\n",
      "Epoch 35/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0926 - accuracy: 0.6977\n",
      "Epoch 36/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0728 - accuracy: 0.7031\n",
      "Epoch 37/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0584 - accuracy: 0.7006\n",
      "Epoch 38/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0439 - accuracy: 0.7026\n",
      "Epoch 39/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0238 - accuracy: 0.7177\n",
      "Epoch 40/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 1.0097 - accuracy: 0.7123\n",
      "Epoch 41/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9962 - accuracy: 0.7201\n",
      "Epoch 42/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9815 - accuracy: 0.7206\n",
      "Epoch 43/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9632 - accuracy: 0.7235\n",
      "Epoch 44/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9489 - accuracy: 0.7328\n",
      "Epoch 45/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9304 - accuracy: 0.7362\n",
      "Epoch 46/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9223 - accuracy: 0.7382\n",
      "Epoch 47/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.9099 - accuracy: 0.7450\n",
      "Epoch 48/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.8921 - accuracy: 0.7504\n",
      "Epoch 49/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.8827 - accuracy: 0.7479\n",
      "Epoch 50/50\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.8720 - accuracy: 0.7640\n",
      "5th iteration done\n",
      "9/9 [==============================] - 0s 4ms/step\n",
      "Validation F1 Score (macro): 0.15513012351597108\n",
      "Validation Accuracy: 0.1684981684981685\n",
      "Shapes before scaling:\n",
      "X_train: (1904, 384)\n",
      "X_test: (420, 384)\n",
      "Epoch 1/50\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.7993 - accuracy: 0.0877\n",
      "Epoch 2/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.6077 - accuracy: 0.1287\n",
      "Epoch 3/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.4774 - accuracy: 0.1728\n",
      "Epoch 4/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3764 - accuracy: 0.2164\n",
      "Epoch 5/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.3206 - accuracy: 0.2453\n",
      "Epoch 6/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.2419 - accuracy: 0.2847\n",
      "Epoch 7/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.1903 - accuracy: 0.2988\n",
      "Epoch 8/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.1217 - accuracy: 0.3477\n",
      "Epoch 9/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.0747 - accuracy: 0.3603\n",
      "Epoch 10/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 2.0235 - accuracy: 0.3803\n",
      "Epoch 11/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.9713 - accuracy: 0.3908\n",
      "Epoch 12/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.9199 - accuracy: 0.4223\n",
      "Epoch 13/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.8757 - accuracy: 0.4459\n",
      "Epoch 14/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.8142 - accuracy: 0.4685\n",
      "Epoch 15/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.7822 - accuracy: 0.4685\n",
      "Epoch 16/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.7293 - accuracy: 0.4842\n",
      "Epoch 17/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.6878 - accuracy: 0.5142\n",
      "Epoch 18/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.6617 - accuracy: 0.5089\n",
      "Epoch 19/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.6136 - accuracy: 0.5226\n",
      "Epoch 20/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.5762 - accuracy: 0.5446\n",
      "Epoch 21/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.5467 - accuracy: 0.5394\n",
      "Epoch 22/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.5090 - accuracy: 0.5567\n",
      "Epoch 23/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.4810 - accuracy: 0.5636\n",
      "Epoch 24/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.4522 - accuracy: 0.5730\n",
      "Epoch 25/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.4104 - accuracy: 0.5998\n",
      "Epoch 26/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.3819 - accuracy: 0.6029\n",
      "Epoch 27/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.3538 - accuracy: 0.6098\n",
      "Epoch 28/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.3355 - accuracy: 0.6082\n",
      "Epoch 29/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.3095 - accuracy: 0.6203\n",
      "Epoch 30/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.2857 - accuracy: 0.6250\n",
      "Epoch 31/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.2641 - accuracy: 0.6334\n",
      "Epoch 32/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.2412 - accuracy: 0.6355\n",
      "Epoch 33/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.2160 - accuracy: 0.6444\n",
      "Epoch 34/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1993 - accuracy: 0.6444\n",
      "Epoch 35/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1767 - accuracy: 0.6612\n",
      "Epoch 36/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1598 - accuracy: 0.6591\n",
      "Epoch 37/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1339 - accuracy: 0.6670\n",
      "Epoch 38/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.1219 - accuracy: 0.6754\n",
      "Epoch 39/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.0933 - accuracy: 0.6864\n",
      "Epoch 40/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 1.0845 - accuracy: 0.6928\n",
      "Epoch 41/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.0695 - accuracy: 0.6996\n",
      "Epoch 42/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.0560 - accuracy: 0.6922\n",
      "Epoch 43/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.0453 - accuracy: 0.6875\n",
      "Epoch 44/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.0169 - accuracy: 0.7022\n",
      "Epoch 45/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.0053 - accuracy: 0.7153\n",
      "Epoch 46/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.9887 - accuracy: 0.7117\n",
      "Epoch 47/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.9810 - accuracy: 0.7153\n",
      "Epoch 48/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.9700 - accuracy: 0.7216\n",
      "Epoch 49/50\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.9488 - accuracy: 0.7201\n",
      "Epoch 50/50\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.9417 - accuracy: 0.7211\n",
      "6th iteration done\n",
      "14/14 [==============================] - 0s 3ms/step\n",
      "Validation F1 Score (macro): 0.23766675862946493\n",
      "Validation Accuracy: 0.2714285714285714\n"
     ]
    }
   ],
   "source": [
    "#try neural nets\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def build_mlp_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(1024, activation='sigmoid', input_shape=(input_shape,)),\n",
    "        Dropout(0.22),\n",
    "        layers.Dense(512, activation='sigmoid'),\n",
    "        Dropout(0.30),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#Remove one speaker for cross validation\n",
    "cols=smile_norm.columns\n",
    "scores_nn=pd.DataFrame(columns=['speaker', 'accuracy', 'F1', 'n'])\n",
    "i=0\n",
    "best_f1=0\n",
    "best_pred=None\n",
    "for speaker in smile_norm['speaker'].unique():\n",
    "    # Filter the data for the current speaker\n",
    "    test = smile_norm[smile_norm['speaker'] == speaker]\n",
    "    # Filter the remaining data for training\n",
    "    train = smile_norm[smile_norm['speaker'] != speaker]\n",
    "    features= list(train.columns[4:])\n",
    "\n",
    "    X_train= train[features]\n",
    "    y_train= train['emotion']\n",
    "\n",
    "    X_test= test[features]\n",
    "    y_test= test['emotion']\n",
    "    \n",
    "    #only OpenSmile\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "    \n",
    "    print(\"Shapes before scaling:\")\n",
    "    print(\"X_train:\", X_train.shape)\n",
    "    print(\"X_test:\", X_test.shape)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = build_mlp_model(input_shape=X_train.shape[1], num_classes=num_classes)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "    print(f\"{i}th iteration done\")\n",
    "    y_val_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_val_pred)\n",
    "    f1 = f1_score(y_test, y_val_pred, average='weighted')\n",
    "    if f1 > best_f1:\n",
    "        best_f1=f1\n",
    "        best_pred=y_val_pred\n",
    "    print(\"Validation F1 Score (macro):\", f1)\n",
    "    print(\"Validation Accuracy:\", accuracy)\n",
    "    n=test.shape[0]\n",
    "    \n",
    "    row=[speaker, accuracy, f1, n]\n",
    "    scores_nn.loc[i]=row\n",
    "    reports.append(classification_report(y_test, y_val_pred, zero_division=0))\n",
    "    \n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cddd1f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mm</td>\n",
       "      <td>0.301325</td>\n",
       "      <td>0.282579</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc</td>\n",
       "      <td>0.256604</td>\n",
       "      <td>0.232944</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mf</td>\n",
       "      <td>0.224080</td>\n",
       "      <td>0.218448</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cl</td>\n",
       "      <td>0.201087</td>\n",
       "      <td>0.186707</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mk</td>\n",
       "      <td>0.193955</td>\n",
       "      <td>0.171204</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jg</td>\n",
       "      <td>0.168498</td>\n",
       "      <td>0.155130</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gg</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.237667</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  speaker  accuracy        F1    n\n",
       "0      mm  0.301325  0.282579  302\n",
       "1      cc  0.256604  0.232944  265\n",
       "2      mf  0.224080  0.218448  299\n",
       "3      cl  0.201087  0.186707  368\n",
       "4      mk  0.193955  0.171204  397\n",
       "5      jg  0.168498  0.155130  273\n",
       "6      gg  0.271429  0.237667  420"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2c0e8ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Average Accuracry:  0.2310671256454389\n",
      "Aggreaged F1:  0.21137334372806474\n"
     ]
    }
   ],
   "source": [
    "assert(scores_nn['n'].sum()==2324)\n",
    "n_total= scores_nn['n'].sum()\n",
    "accuracy=0\n",
    "F1=0\n",
    "#will run 7 times\n",
    "for index, row in scores_nn.iterrows():\n",
    "    speaker= row['speaker']\n",
    "    acc= row['accuracy']\n",
    "    f1= row['F1']\n",
    "    n= row['n']\n",
    "    accuracy+= acc * n\n",
    "    F1+= f1 * n\n",
    "\n",
    "aggregated_accuracy= accuracy/n_total\n",
    "aggregated_F1= F1/n_total\n",
    "\n",
    "print(\"Aggregated Average Accuracry: \", aggregated_accuracy)\n",
    "print(\"Aggreaged F1: \", aggregated_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "eee890c5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxiety       0.57      0.33      0.42        39\n",
      "     boredom       0.12      0.05      0.07        19\n",
      "  cold-anger       0.10      0.05      0.07        20\n",
      "    contempt       0.10      0.05      0.07        19\n",
      "     despair       0.14      0.11      0.12        18\n",
      "     disgust       0.29      0.26      0.27        23\n",
      "     elation       0.31      0.21      0.25        19\n",
      "       happy       0.39      0.50      0.44        18\n",
      "   hot-anger       0.40      0.38      0.39        16\n",
      "    interest       0.24      0.48      0.32        21\n",
      "     neutral       0.00      0.00      0.00         9\n",
      "       panic       0.44      0.14      0.22        28\n",
      "       pride       0.27      0.21      0.24        19\n",
      "     sadness       0.12      0.29      0.17        17\n",
      "       shame       0.23      0.76      0.35        17\n",
      "\n",
      "    accuracy                           0.26       302\n",
      "   macro avg       0.25      0.26      0.23       302\n",
      "weighted avg       0.28      0.26      0.24       302\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxiety       0.04      0.30      0.08        10\n",
      "     boredom       0.17      0.07      0.10        15\n",
      "  cold-anger       0.04      0.07      0.05        15\n",
      "    contempt       0.21      0.27      0.24        22\n",
      "     despair       0.00      0.00      0.00         9\n",
      "     disgust       0.25      0.03      0.06        31\n",
      "     elation       0.08      0.12      0.10        16\n",
      "       happy       0.24      0.17      0.20        23\n",
      "   hot-anger       0.53      0.64      0.58        14\n",
      "    interest       0.30      0.18      0.22        17\n",
      "     neutral       0.67      0.44      0.53        18\n",
      "       panic       0.50      0.44      0.47        18\n",
      "       pride       0.22      0.09      0.12        23\n",
      "     sadness       0.20      0.15      0.17        13\n",
      "       shame       0.38      0.24      0.29        21\n",
      "\n",
      "    accuracy                           0.21       265\n",
      "   macro avg       0.26      0.21      0.21       265\n",
      "weighted avg       0.27      0.21      0.22       265\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxiety       0.17      0.41      0.24        22\n",
      "     boredom       0.19      0.30      0.23        27\n",
      "  cold-anger       0.00      0.00      0.00        20\n",
      "    contempt       0.50      0.09      0.15        44\n",
      "     despair       0.00      0.00      0.00        16\n",
      "     disgust       0.00      0.00      0.00         1\n",
      "     elation       0.12      0.08      0.09        26\n",
      "       happy       0.05      0.09      0.07        23\n",
      "   hot-anger       0.20      0.10      0.13        21\n",
      "    interest       0.26      0.37      0.30        19\n",
      "     neutral       0.30      0.30      0.30        10\n",
      "       panic       0.25      0.50      0.33        12\n",
      "       pride       0.14      0.17      0.15        18\n",
      "     sadness       0.00      0.00      0.00        20\n",
      "       shame       0.29      0.25      0.27        20\n",
      "\n",
      "    accuracy                           0.17       299\n",
      "   macro avg       0.16      0.18      0.15       299\n",
      "weighted avg       0.20      0.17      0.15       299\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxiety       0.14      0.43      0.21        21\n",
      "     boredom       0.30      0.83      0.44        29\n",
      "  cold-anger       0.44      0.15      0.22        27\n",
      "    contempt       0.12      0.08      0.10        25\n",
      "     despair       0.20      0.10      0.14        29\n",
      "     disgust       0.00      0.00      0.00        22\n",
      "     elation       0.75      0.22      0.34        27\n",
      "       happy       0.60      0.14      0.23        21\n",
      "   hot-anger       0.24      0.15      0.19        26\n",
      "    interest       0.12      0.04      0.06        26\n",
      "     neutral       0.00      0.00      0.00        17\n",
      "       panic       0.00      0.00      0.00        21\n",
      "       pride       0.50      0.17      0.25        24\n",
      "     sadness       0.13      0.26      0.17        27\n",
      "       shame       0.02      0.04      0.03        26\n",
      "\n",
      "    accuracy                           0.18       368\n",
      "   macro avg       0.24      0.17      0.16       368\n",
      "weighted avg       0.25      0.18      0.17       368\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxiety       0.17      0.07      0.10        29\n",
      "     boredom       0.07      0.05      0.06        20\n",
      "  cold-anger       0.13      0.26      0.17        23\n",
      "    contempt       0.13      0.29      0.18        21\n",
      "     despair       0.00      0.00      0.00        53\n",
      "     disgust       0.05      0.05      0.05        21\n",
      "     elation       0.11      0.30      0.16        23\n",
      "       happy       0.22      0.31      0.25        42\n",
      "   hot-anger       0.26      0.59      0.36        22\n",
      "    interest       0.13      0.11      0.12        44\n",
      "     neutral       0.00      0.00      0.00         8\n",
      "       panic       0.36      0.19      0.25        21\n",
      "       pride       0.00      0.00      0.00        23\n",
      "     sadness       0.00      0.00      0.00        22\n",
      "       shame       0.40      0.16      0.23        25\n",
      "\n",
      "    accuracy                           0.16       397\n",
      "   macro avg       0.13      0.16      0.13       397\n",
      "weighted avg       0.13      0.16      0.13       397\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxiety       0.14      0.16      0.15        19\n",
      "     boredom       0.12      0.07      0.09        14\n",
      "  cold-anger       0.11      0.18      0.14        22\n",
      "    contempt       0.06      0.09      0.07        23\n",
      "     despair       0.17      0.24      0.20        21\n",
      "     disgust       0.21      0.30      0.25        23\n",
      "     elation       0.31      0.20      0.24        20\n",
      "       happy       0.21      0.20      0.21        20\n",
      "   hot-anger       0.33      0.33      0.33        18\n",
      "    interest       0.13      0.21      0.16        19\n",
      "     neutral       0.00      0.00      0.00         8\n",
      "       panic       0.00      0.00      0.00        14\n",
      "       pride       0.00      0.00      0.00        18\n",
      "     sadness       0.50      0.05      0.10        19\n",
      "       shame       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.15       273\n",
      "   macro avg       0.15      0.14      0.13       273\n",
      "weighted avg       0.16      0.15      0.14       273\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxiety       0.38      0.37      0.37        30\n",
      "     boredom       0.17      0.03      0.06        30\n",
      "  cold-anger       0.24      0.33      0.28        27\n",
      "    contempt       0.30      0.31      0.30        26\n",
      "     despair       0.00      0.00      0.00        28\n",
      "     disgust       0.22      0.04      0.07        51\n",
      "     elation       0.23      0.57      0.33        28\n",
      "       happy       0.14      0.30      0.19        30\n",
      "   hot-anger       0.25      0.59      0.35        22\n",
      "    interest       0.13      0.13      0.13        30\n",
      "     neutral       1.00      0.11      0.20         9\n",
      "       panic       0.36      0.63      0.46        27\n",
      "       pride       0.12      0.12      0.12        25\n",
      "     sadness       0.00      0.00      0.00        33\n",
      "       shame       0.31      0.21      0.25        24\n",
      "\n",
      "    accuracy                           0.24       420\n",
      "   macro avg       0.26      0.25      0.21       420\n",
      "weighted avg       0.22      0.24      0.19       420\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.33      0.41        39\n",
      "           1       0.50      0.11      0.17        19\n",
      "           2       0.12      0.05      0.07        20\n",
      "           3       0.00      0.00      0.00        19\n",
      "           4       0.20      0.33      0.25        18\n",
      "           5       0.18      0.17      0.18        23\n",
      "           6       0.29      0.53      0.38        19\n",
      "           7       0.39      0.50      0.44        18\n",
      "           8       0.33      0.19      0.24        16\n",
      "           9       0.22      0.57      0.32        21\n",
      "          10       0.67      0.22      0.33         9\n",
      "          11       0.46      0.43      0.44        28\n",
      "          12       0.22      0.11      0.14        19\n",
      "          13       0.14      0.29      0.19        17\n",
      "          14       0.50      0.47      0.48        17\n",
      "\n",
      "    accuracy                           0.29       302\n",
      "   macro avg       0.32      0.29      0.27       302\n",
      "weighted avg       0.32      0.29      0.28       302\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.10      0.07        10\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.26      0.55      0.35        22\n",
      "           4       0.25      0.11      0.15         9\n",
      "           5       0.33      0.10      0.15        31\n",
      "           6       0.31      0.50      0.38        16\n",
      "           7       0.10      0.04      0.06        23\n",
      "           8       0.28      0.36      0.31        14\n",
      "           9       0.32      0.35      0.33        17\n",
      "          10       0.60      0.50      0.55        18\n",
      "          11       0.29      0.78      0.42        18\n",
      "          12       0.00      0.00      0.00        23\n",
      "          13       0.24      0.31      0.27        13\n",
      "          14       0.15      0.10      0.12        21\n",
      "\n",
      "    accuracy                           0.25       265\n",
      "   macro avg       0.21      0.25      0.21       265\n",
      "weighted avg       0.22      0.25      0.21       265\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.14      0.18        22\n",
      "           1       0.24      0.44      0.31        27\n",
      "           2       0.10      0.15      0.12        20\n",
      "           3       0.43      0.14      0.21        44\n",
      "           4       0.00      0.00      0.00        16\n",
      "           5       0.04      1.00      0.07         1\n",
      "           6       0.19      0.15      0.17        26\n",
      "           7       0.00      0.00      0.00        23\n",
      "           8       0.58      0.33      0.42        21\n",
      "           9       0.33      0.32      0.32        19\n",
      "          10       0.45      1.00      0.62        10\n",
      "          11       0.50      0.83      0.62        12\n",
      "          12       0.12      0.22      0.15        18\n",
      "          13       0.00      0.00      0.00        20\n",
      "          14       0.31      0.25      0.28        20\n",
      "\n",
      "    accuracy                           0.24       299\n",
      "   macro avg       0.24      0.33      0.23       299\n",
      "weighted avg       0.25      0.24      0.22       299\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.19      0.20        21\n",
      "           1       0.25      0.69      0.37        29\n",
      "           2       0.00      0.00      0.00        27\n",
      "           3       0.13      0.20      0.16        25\n",
      "           4       0.09      0.07      0.08        29\n",
      "           5       0.02      0.05      0.03        22\n",
      "           6       0.62      0.59      0.60        27\n",
      "           7       0.43      0.14      0.21        21\n",
      "           8       0.71      0.19      0.30        26\n",
      "           9       0.20      0.04      0.06        26\n",
      "          10       0.00      0.00      0.00        17\n",
      "          11       0.75      0.14      0.24        21\n",
      "          12       1.00      0.12      0.22        24\n",
      "          13       0.10      0.19      0.13        27\n",
      "          14       0.07      0.15      0.10        26\n",
      "\n",
      "    accuracy                           0.20       368\n",
      "   macro avg       0.31      0.18      0.18       368\n",
      "weighted avg       0.30      0.20      0.19       368\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.03      0.05        29\n",
      "           1       0.00      0.00      0.00        20\n",
      "           2       0.21      0.39      0.28        23\n",
      "           3       0.19      0.33      0.24        21\n",
      "           4       0.24      0.08      0.11        53\n",
      "           5       0.00      0.00      0.00        21\n",
      "           6       0.14      0.26      0.18        23\n",
      "           7       0.21      0.36      0.27        42\n",
      "           8       0.05      0.05      0.05        22\n",
      "           9       0.21      0.41      0.28        44\n",
      "          10       1.00      0.25      0.40         8\n",
      "          11       0.19      0.14      0.16        21\n",
      "          12       0.00      0.00      0.00        23\n",
      "          13       0.00      0.00      0.00        22\n",
      "          14       0.26      0.28      0.27        25\n",
      "\n",
      "    accuracy                           0.18       397\n",
      "   macro avg       0.19      0.17      0.15       397\n",
      "weighted avg       0.16      0.18      0.15       397\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.11      0.11        19\n",
      "           1       0.25      0.36      0.29        14\n",
      "           2       0.09      0.18      0.12        22\n",
      "           3       0.17      0.13      0.15        23\n",
      "           4       0.13      0.19      0.16        21\n",
      "           5       0.25      0.35      0.29        23\n",
      "           6       0.33      0.05      0.09        20\n",
      "           7       0.00      0.00      0.00        20\n",
      "           8       0.26      0.50      0.35        18\n",
      "           9       0.14      0.16      0.15        19\n",
      "          10       0.00      0.00      0.00         8\n",
      "          11       0.33      0.07      0.12        14\n",
      "          12       0.08      0.11      0.09        18\n",
      "          13       0.14      0.05      0.08        19\n",
      "          14       0.33      0.13      0.19        15\n",
      "\n",
      "    accuracy                           0.16       273\n",
      "   macro avg       0.18      0.16      0.15       273\n",
      "weighted avg       0.18      0.16      0.15       273\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.33      0.39        30\n",
      "           1       0.20      0.07      0.10        30\n",
      "           2       0.19      0.19      0.19        27\n",
      "           3       0.26      0.27      0.26        26\n",
      "           4       0.00      0.00      0.00        28\n",
      "           5       0.17      0.02      0.04        51\n",
      "           6       0.27      0.68      0.38        28\n",
      "           7       0.19      0.60      0.28        30\n",
      "           8       0.19      0.36      0.25        22\n",
      "           9       0.43      0.20      0.27        30\n",
      "          10       0.20      0.11      0.14         9\n",
      "          11       0.54      0.48      0.51        27\n",
      "          12       0.09      0.16      0.12        25\n",
      "          13       0.00      0.00      0.00        33\n",
      "          14       0.32      0.33      0.33        24\n",
      "\n",
      "    accuracy                           0.24       420\n",
      "   macro avg       0.23      0.25      0.22       420\n",
      "weighted avg       0.23      0.24      0.21       420\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.31      0.36        39\n",
      "           1       0.50      0.11      0.17        19\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.00      0.00      0.00        19\n",
      "           4       0.17      0.33      0.23        18\n",
      "           5       0.25      0.17      0.21        23\n",
      "           6       0.28      0.79      0.41        19\n",
      "           7       0.25      0.22      0.24        18\n",
      "           8       0.40      0.12      0.19        16\n",
      "           9       0.24      0.48      0.32        21\n",
      "          10       0.75      0.33      0.46         9\n",
      "          11       0.65      0.39      0.49        28\n",
      "          12       0.11      0.05      0.07        19\n",
      "          13       0.13      0.35      0.19        17\n",
      "          14       0.42      0.47      0.44        17\n",
      "\n",
      "    accuracy                           0.28       302\n",
      "   macro avg       0.31      0.28      0.25       302\n",
      "weighted avg       0.31      0.28      0.26       302\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.10      0.06        10\n",
      "           1       0.04      0.07      0.05        15\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.28      0.50      0.35        22\n",
      "           4       0.40      0.22      0.29         9\n",
      "           5       0.36      0.13      0.19        31\n",
      "           6       0.23      0.31      0.26        16\n",
      "           7       0.10      0.04      0.06        23\n",
      "           8       0.31      0.57      0.40        14\n",
      "           9       0.39      0.41      0.40        17\n",
      "          10       0.57      0.44      0.50        18\n",
      "          11       0.33      0.72      0.46        18\n",
      "          12       0.25      0.04      0.07        23\n",
      "          13       0.27      0.31      0.29        13\n",
      "          14       0.18      0.10      0.12        21\n",
      "\n",
      "    accuracy                           0.26       265\n",
      "   macro avg       0.25      0.26      0.23       265\n",
      "weighted avg       0.26      0.26      0.23       265\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.14      0.18        22\n",
      "           1       0.22      0.37      0.27        27\n",
      "           2       0.14      0.20      0.16        20\n",
      "           3       0.44      0.18      0.26        44\n",
      "           4       0.00      0.00      0.00        16\n",
      "           5       0.04      1.00      0.07         1\n",
      "           6       0.20      0.12      0.15        26\n",
      "           7       0.11      0.09      0.10        23\n",
      "           8       0.57      0.38      0.46        21\n",
      "           9       0.22      0.32      0.26        19\n",
      "          10       0.50      1.00      0.67        10\n",
      "          11       0.43      0.83      0.57        12\n",
      "          12       0.06      0.06      0.06        18\n",
      "          13       0.00      0.00      0.00        20\n",
      "          14       0.41      0.35      0.38        20\n",
      "\n",
      "    accuracy                           0.24       299\n",
      "   macro avg       0.24      0.34      0.24       299\n",
      "weighted avg       0.26      0.24      0.23       299\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.10      0.11        21\n",
      "           1       0.26      0.76      0.39        29\n",
      "           2       0.33      0.04      0.07        27\n",
      "           3       0.17      0.20      0.19        25\n",
      "           4       0.09      0.03      0.05        29\n",
      "           5       0.02      0.05      0.03        22\n",
      "           6       0.67      0.52      0.58        27\n",
      "           7       0.57      0.19      0.29        21\n",
      "           8       0.71      0.19      0.30        26\n",
      "           9       0.25      0.04      0.07        26\n",
      "          10       0.00      0.00      0.00        17\n",
      "          11       0.75      0.14      0.24        21\n",
      "          12       1.00      0.08      0.15        24\n",
      "          13       0.10      0.19      0.13        27\n",
      "          14       0.08      0.23      0.12        26\n",
      "\n",
      "    accuracy                           0.20       368\n",
      "   macro avg       0.34      0.18      0.18       368\n",
      "weighted avg       0.34      0.20      0.19       368\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.03      0.05        29\n",
      "           1       0.08      0.05      0.06        20\n",
      "           2       0.18      0.52      0.27        23\n",
      "           3       0.15      0.19      0.17        21\n",
      "           4       0.19      0.08      0.11        53\n",
      "           5       0.00      0.00      0.00        21\n",
      "           6       0.10      0.26      0.15        23\n",
      "           7       0.23      0.24      0.23        42\n",
      "           8       0.00      0.00      0.00        22\n",
      "           9       0.23      0.39      0.29        44\n",
      "          10       1.00      0.25      0.40         8\n",
      "          11       0.14      0.10      0.11        21\n",
      "          12       0.00      0.00      0.00        23\n",
      "          13       0.14      0.05      0.07        22\n",
      "          14       0.26      0.24      0.25        25\n",
      "\n",
      "    accuracy                           0.17       397\n",
      "   macro avg       0.19      0.16      0.14       397\n",
      "weighted avg       0.17      0.17      0.15       397\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.16      0.15        19\n",
      "           1       0.50      0.36      0.42        14\n",
      "           2       0.12      0.18      0.15        22\n",
      "           3       0.13      0.13      0.13        23\n",
      "           4       0.11      0.24      0.15        21\n",
      "           5       0.20      0.22      0.21        23\n",
      "           6       0.00      0.00      0.00        20\n",
      "           7       0.00      0.00      0.00        20\n",
      "           8       0.23      0.50      0.32        18\n",
      "           9       0.19      0.16      0.17        19\n",
      "          10       0.00      0.00      0.00         8\n",
      "          11       0.50      0.14      0.22        14\n",
      "          12       0.14      0.17      0.15        18\n",
      "          13       0.17      0.11      0.13        19\n",
      "          14       0.33      0.27      0.30        15\n",
      "\n",
      "    accuracy                           0.18       273\n",
      "   macro avg       0.18      0.17      0.17       273\n",
      "weighted avg       0.18      0.18      0.16       273\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.37      0.42        30\n",
      "           1       0.27      0.13      0.18        30\n",
      "           2       0.25      0.48      0.33        27\n",
      "           3       0.30      0.23      0.26        26\n",
      "           4       0.00      0.00      0.00        28\n",
      "           5       0.14      0.04      0.06        51\n",
      "           6       0.27      0.61      0.37        28\n",
      "           7       0.18      0.50      0.27        30\n",
      "           8       0.22      0.45      0.30        22\n",
      "           9       0.38      0.17      0.23        30\n",
      "          10       0.20      0.11      0.14         9\n",
      "          11       0.55      0.41      0.47        27\n",
      "          12       0.16      0.20      0.18        25\n",
      "          13       0.00      0.00      0.00        33\n",
      "          14       0.28      0.29      0.29        24\n",
      "\n",
      "    accuracy                           0.25       420\n",
      "   macro avg       0.25      0.27      0.23       420\n",
      "weighted avg       0.24      0.25      0.22       420\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.33      0.40        39\n",
      "           1       0.40      0.11      0.17        19\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.00      0.00      0.00        19\n",
      "           4       0.18      0.33      0.23        18\n",
      "           5       0.27      0.17      0.21        23\n",
      "           6       0.28      0.68      0.40        19\n",
      "           7       0.28      0.28      0.28        18\n",
      "           8       0.43      0.19      0.26        16\n",
      "           9       0.22      0.52      0.31        21\n",
      "          10       1.00      0.33      0.50         9\n",
      "          11       0.52      0.46      0.49        28\n",
      "          12       0.33      0.16      0.21        19\n",
      "          13       0.14      0.24      0.17        17\n",
      "          14       0.46      0.65      0.54        17\n",
      "\n",
      "    accuracy                           0.30       302\n",
      "   macro avg       0.33      0.30      0.28       302\n",
      "weighted avg       0.33      0.30      0.28       302\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.10      0.07        10\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.32      0.50      0.39        22\n",
      "           4       0.33      0.22      0.27         9\n",
      "           5       0.50      0.13      0.21        31\n",
      "           6       0.25      0.38      0.30        16\n",
      "           7       0.17      0.09      0.11        23\n",
      "           8       0.30      0.57      0.39        14\n",
      "           9       0.31      0.47      0.37        17\n",
      "          10       0.70      0.39      0.50        18\n",
      "          11       0.29      0.61      0.39        18\n",
      "          12       0.50      0.04      0.08        23\n",
      "          13       0.29      0.31      0.30        13\n",
      "          14       0.12      0.14      0.13        21\n",
      "\n",
      "    accuracy                           0.26       265\n",
      "   macro avg       0.27      0.26      0.23       265\n",
      "weighted avg       0.30      0.26      0.23       265\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.14      0.16        22\n",
      "           1       0.19      0.26      0.22        27\n",
      "           2       0.10      0.10      0.10        20\n",
      "           3       0.50      0.16      0.24        44\n",
      "           4       0.00      0.00      0.00        16\n",
      "           5       0.02      1.00      0.05         1\n",
      "           6       0.15      0.08      0.10        26\n",
      "           7       0.12      0.09      0.10        23\n",
      "           8       0.62      0.48      0.54        21\n",
      "           9       0.33      0.32      0.32        19\n",
      "          10       0.48      1.00      0.65        10\n",
      "          11       0.43      0.75      0.55        12\n",
      "          12       0.05      0.06      0.05        18\n",
      "          13       0.00      0.00      0.00        20\n",
      "          14       0.32      0.35      0.33        20\n",
      "\n",
      "    accuracy                           0.22       299\n",
      "   macro avg       0.24      0.32      0.23       299\n",
      "weighted avg       0.26      0.22      0.22       299\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.19      0.17        21\n",
      "           1       0.32      0.72      0.44        29\n",
      "           2       0.00      0.00      0.00        27\n",
      "           3       0.17      0.20      0.18        25\n",
      "           4       0.00      0.00      0.00        29\n",
      "           5       0.03      0.05      0.04        22\n",
      "           6       0.59      0.59      0.59        27\n",
      "           7       0.57      0.19      0.29        21\n",
      "           8       0.75      0.23      0.35        26\n",
      "           9       0.00      0.00      0.00        26\n",
      "          10       0.00      0.00      0.00        17\n",
      "          11       1.00      0.14      0.25        21\n",
      "          12       0.75      0.12      0.21        24\n",
      "          13       0.06      0.11      0.08        27\n",
      "          14       0.08      0.31      0.13        26\n",
      "\n",
      "    accuracy                           0.20       368\n",
      "   macro avg       0.30      0.19      0.18       368\n",
      "weighted avg       0.29      0.20      0.19       368\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.03      0.05        29\n",
      "           1       0.10      0.05      0.07        20\n",
      "           2       0.25      0.52      0.34        23\n",
      "           3       0.11      0.33      0.17        21\n",
      "           4       0.24      0.09      0.14        53\n",
      "           5       0.00      0.00      0.00        21\n",
      "           6       0.14      0.22      0.17        23\n",
      "           7       0.25      0.36      0.29        42\n",
      "           8       0.09      0.09      0.09        22\n",
      "           9       0.25      0.39      0.30        44\n",
      "          10       1.00      0.12      0.22         8\n",
      "          11       0.21      0.19      0.20        21\n",
      "          12       0.00      0.00      0.00        23\n",
      "          13       0.20      0.05      0.07        22\n",
      "          14       0.40      0.24      0.30        25\n",
      "\n",
      "    accuracy                           0.19       397\n",
      "   macro avg       0.22      0.18      0.16       397\n",
      "weighted avg       0.20      0.19      0.17       397\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.16      0.15        19\n",
      "           1       0.29      0.43      0.34        14\n",
      "           2       0.13      0.18      0.15        22\n",
      "           3       0.13      0.13      0.13        23\n",
      "           4       0.11      0.19      0.14        21\n",
      "           5       0.22      0.22      0.22        23\n",
      "           6       0.33      0.05      0.09        20\n",
      "           7       0.00      0.00      0.00        20\n",
      "           8       0.22      0.44      0.30        18\n",
      "           9       0.15      0.16      0.15        19\n",
      "          10       0.00      0.00      0.00         8\n",
      "          11       0.50      0.14      0.22        14\n",
      "          12       0.13      0.22      0.17        18\n",
      "          13       0.14      0.05      0.08        19\n",
      "          14       0.29      0.13      0.18        15\n",
      "\n",
      "    accuracy                           0.17       273\n",
      "   macro avg       0.19      0.17      0.15       273\n",
      "weighted avg       0.18      0.17      0.16       273\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.43      0.44        30\n",
      "           1       0.36      0.17      0.23        30\n",
      "           2       0.27      0.48      0.35        27\n",
      "           3       0.33      0.27      0.30        26\n",
      "           4       0.00      0.00      0.00        28\n",
      "           5       0.14      0.02      0.03        51\n",
      "           6       0.23      0.57      0.33        28\n",
      "           7       0.23      0.53      0.32        30\n",
      "           8       0.22      0.36      0.27        22\n",
      "           9       0.50      0.20      0.29        30\n",
      "          10       0.33      0.22      0.27         9\n",
      "          11       0.43      0.48      0.46        27\n",
      "          12       0.12      0.20      0.15        25\n",
      "          13       0.00      0.00      0.00        33\n",
      "          14       0.39      0.38      0.38        24\n",
      "\n",
      "    accuracy                           0.27       420\n",
      "   macro avg       0.27      0.29      0.25       420\n",
      "weighted avg       0.26      0.27      0.24       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#All classification reports, even earlier ones\n",
    "for report in reports:\n",
    "    print((report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "98f6c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.33      0.40        39\n",
      "           1       0.40      0.11      0.17        19\n",
      "           2       0.00      0.00      0.00        20\n",
      "           3       0.00      0.00      0.00        19\n",
      "           4       0.18      0.33      0.23        18\n",
      "           5       0.27      0.17      0.21        23\n",
      "           6       0.28      0.68      0.40        19\n",
      "           7       0.28      0.28      0.28        18\n",
      "           8       0.43      0.19      0.26        16\n",
      "           9       0.22      0.52      0.31        21\n",
      "          10       1.00      0.33      0.50         9\n",
      "          11       0.52      0.46      0.49        28\n",
      "          12       0.33      0.16      0.21        19\n",
      "          13       0.14      0.24      0.17        17\n",
      "          14       0.46      0.65      0.54        17\n",
      "\n",
      "    accuracy                           0.30       302\n",
      "   macro avg       0.33      0.30      0.28       302\n",
      "weighted avg       0.33      0.30      0.28       302\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.10      0.07        10\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.32      0.50      0.39        22\n",
      "           4       0.33      0.22      0.27         9\n",
      "           5       0.50      0.13      0.21        31\n",
      "           6       0.25      0.38      0.30        16\n",
      "           7       0.17      0.09      0.11        23\n",
      "           8       0.30      0.57      0.39        14\n",
      "           9       0.31      0.47      0.37        17\n",
      "          10       0.70      0.39      0.50        18\n",
      "          11       0.29      0.61      0.39        18\n",
      "          12       0.50      0.04      0.08        23\n",
      "          13       0.29      0.31      0.30        13\n",
      "          14       0.12      0.14      0.13        21\n",
      "\n",
      "    accuracy                           0.26       265\n",
      "   macro avg       0.27      0.26      0.23       265\n",
      "weighted avg       0.30      0.26      0.23       265\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.14      0.16        22\n",
      "           1       0.19      0.26      0.22        27\n",
      "           2       0.10      0.10      0.10        20\n",
      "           3       0.50      0.16      0.24        44\n",
      "           4       0.00      0.00      0.00        16\n",
      "           5       0.02      1.00      0.05         1\n",
      "           6       0.15      0.08      0.10        26\n",
      "           7       0.12      0.09      0.10        23\n",
      "           8       0.62      0.48      0.54        21\n",
      "           9       0.33      0.32      0.32        19\n",
      "          10       0.48      1.00      0.65        10\n",
      "          11       0.43      0.75      0.55        12\n",
      "          12       0.05      0.06      0.05        18\n",
      "          13       0.00      0.00      0.00        20\n",
      "          14       0.32      0.35      0.33        20\n",
      "\n",
      "    accuracy                           0.22       299\n",
      "   macro avg       0.24      0.32      0.23       299\n",
      "weighted avg       0.26      0.22      0.22       299\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.19      0.17        21\n",
      "           1       0.32      0.72      0.44        29\n",
      "           2       0.00      0.00      0.00        27\n",
      "           3       0.17      0.20      0.18        25\n",
      "           4       0.00      0.00      0.00        29\n",
      "           5       0.03      0.05      0.04        22\n",
      "           6       0.59      0.59      0.59        27\n",
      "           7       0.57      0.19      0.29        21\n",
      "           8       0.75      0.23      0.35        26\n",
      "           9       0.00      0.00      0.00        26\n",
      "          10       0.00      0.00      0.00        17\n",
      "          11       1.00      0.14      0.25        21\n",
      "          12       0.75      0.12      0.21        24\n",
      "          13       0.06      0.11      0.08        27\n",
      "          14       0.08      0.31      0.13        26\n",
      "\n",
      "    accuracy                           0.20       368\n",
      "   macro avg       0.30      0.19      0.18       368\n",
      "weighted avg       0.29      0.20      0.19       368\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.03      0.05        29\n",
      "           1       0.10      0.05      0.07        20\n",
      "           2       0.25      0.52      0.34        23\n",
      "           3       0.11      0.33      0.17        21\n",
      "           4       0.24      0.09      0.14        53\n",
      "           5       0.00      0.00      0.00        21\n",
      "           6       0.14      0.22      0.17        23\n",
      "           7       0.25      0.36      0.29        42\n",
      "           8       0.09      0.09      0.09        22\n",
      "           9       0.25      0.39      0.30        44\n",
      "          10       1.00      0.12      0.22         8\n",
      "          11       0.21      0.19      0.20        21\n",
      "          12       0.00      0.00      0.00        23\n",
      "          13       0.20      0.05      0.07        22\n",
      "          14       0.40      0.24      0.30        25\n",
      "\n",
      "    accuracy                           0.19       397\n",
      "   macro avg       0.22      0.18      0.16       397\n",
      "weighted avg       0.20      0.19      0.17       397\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.16      0.15        19\n",
      "           1       0.29      0.43      0.34        14\n",
      "           2       0.13      0.18      0.15        22\n",
      "           3       0.13      0.13      0.13        23\n",
      "           4       0.11      0.19      0.14        21\n",
      "           5       0.22      0.22      0.22        23\n",
      "           6       0.33      0.05      0.09        20\n",
      "           7       0.00      0.00      0.00        20\n",
      "           8       0.22      0.44      0.30        18\n",
      "           9       0.15      0.16      0.15        19\n",
      "          10       0.00      0.00      0.00         8\n",
      "          11       0.50      0.14      0.22        14\n",
      "          12       0.13      0.22      0.17        18\n",
      "          13       0.14      0.05      0.08        19\n",
      "          14       0.29      0.13      0.18        15\n",
      "\n",
      "    accuracy                           0.17       273\n",
      "   macro avg       0.19      0.17      0.15       273\n",
      "weighted avg       0.18      0.17      0.16       273\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.43      0.44        30\n",
      "           1       0.36      0.17      0.23        30\n",
      "           2       0.27      0.48      0.35        27\n",
      "           3       0.33      0.27      0.30        26\n",
      "           4       0.00      0.00      0.00        28\n",
      "           5       0.14      0.02      0.03        51\n",
      "           6       0.23      0.57      0.33        28\n",
      "           7       0.23      0.53      0.32        30\n",
      "           8       0.22      0.36      0.27        22\n",
      "           9       0.50      0.20      0.29        30\n",
      "          10       0.33      0.22      0.27         9\n",
      "          11       0.43      0.48      0.46        27\n",
      "          12       0.12      0.20      0.15        25\n",
      "          13       0.00      0.00      0.00        33\n",
      "          14       0.39      0.38      0.38        24\n",
      "\n",
      "    accuracy                           0.27       420\n",
      "   macro avg       0.27      0.29      0.25       420\n",
      "weighted avg       0.26      0.27      0.24       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_reports=reports[21:]\n",
    "for report in best_reports:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "12bceba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for speaker mm (best speaker performance)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#Confusion Matrix\n",
    "emotions=['anxiety', 'boredom', 'cold-anger', 'contempt', 'despair', 'disgust', 'elation', 'happy', 'hot-anger', 'interest', 'neutral', 'panic', 'pride', 'sadness', 'shame']\n",
    "#Generate the confusion matrix\n",
    "#Need to decode labels from numbers into actual tags\n",
    "y_test = smile_norm[smile_norm['speaker'] == 'mm']\n",
    "y_test= y_test['emotion']\n",
    "y_test=label_encoder.transform(y_test)\n",
    "y_true= label_encoder.inverse_transform(y_test)\n",
    "y_pred= label_encoder.inverse_transform(best_pred)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=emotions, normalize=None)\n",
    "\n",
    "row_cm_normalized = confusion_matrix(y_true, y_pred, labels=emotions, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "84f671b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cm=row_cm_normalized\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=emotions)\n",
    "disp.plot(cmap=plt.cm.Reds)\n",
    "\n",
    "\n",
    "plt.title('Row-Normalized Confusion Matrix', fontsize=15, pad=20)\n",
    "plt.xlabel('Prediction', fontsize=11)\n",
    "plt.ylabel('Actual', fontsize=11)\n",
    "\n",
    "\n",
    "#get current axis\n",
    "plt.gca().xaxis.set_label_position('top')\n",
    "plt.gca().xaxis.tick_top()\n",
    "plt.gca().set_aspect('auto')\n",
    "plt.subplots_adjust(left=12, right=14, bottom=0.2)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
